%
% File acl2017.tex
%

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{pgfplotstable}
\usepackage{algorithm2e}
\usepackage{hhline}
\usepackage{multirow}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{float}
\usepackage{lipsum,adjustbox}
\usepackage{tikz}
\usepackage{tikz-dependency}
\usepackage{enumitem}
\usetikzlibrary{shapes,fit,calc,er,positioning,intersections,decorations.shapes,mindmap,trees}
\tikzset{decorate sep/.style 2 args={decorate,decoration={shape backgrounds,shape=circle,
      shape size=#1,shape sep=#2}}}
\newcommand{\oa}[1]{\footnote{\color{red} #1}}
\newcommand{\daniel}[1]{\footnote{\color{blue} #1}}
\newcommand{\com}[1]{}
\newcommand{\parser}[1]{TUPA\textsubscript{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\SetKwRepeat{Do}{do}{while}

\hyphenation{SemEval}
\hyphenation{PARSEVAL}
\hyphenation{DAGParser}
\hyphenation{TurboSemanticParser}
\hyphenation{MaltParser}

\def\linkspace#1#2{\leavevmode
\def\tmp##1{\nolinebreak[2]\href{#1}{\hbox{##1}}}%
\xlinkspace#2 \relax}

\def\xlinkspace#1 #2{%
 \ifx\relax#2%
 \xlinkdash#1-\relax
 \else
 \xlinkdash#1 -\relax
 \expandafter\xlinkspace\expandafter#2%
 \fi}

\def\xlinkdash#1-#2{%
 \ifx\relax#2%
 \tmp{#1}%
 \else
 \tmp{#1-}%
 \expandafter\xlinkdash\expandafter#2%
 \fi}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{Broad-Coverage Transition-Based UCCA Parsing}

\author{Daniel Hershcovich$^{1,2}$ \And Omri Abend$^2$ \And Ari Rappoport$^2$ \\
  $^1$Edmond and Lily Safra Center for Brain Sciences, Hebrew University of Jerusalem \\
  $^2$School of Computer Science and Engineering, Hebrew University of Jerusalem \\
  \texttt{\{danielh,oabend,arir\}@cs.huji.ac.il}
}

\date{}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%     Abstract     %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  We present the first parser for UCCA, a
  cross-linguistically applicable framework for semantic
  representation, which builds on extensive
  typological work, and supports rapid annotation.
  UCCA poses a challenge for existing parsing techniques,
  as it exhibits reentrancy (resulting in DAG structures),
  discontinuous structures and non-terminal nodes corresponding
  to complex semantic units. To our knowledge, the conjunction
  of these formal properties is not supported by any existing parser.
  Our transition-based parser, using a novel transition set
  and features based on bidirectional LSTMs,
  has value not just for UCCA parsing:
  its ability to handle more general graph structures will inform
  the development of parsers for other semantic DAG structures, 
  and in languages that frequently use discontinuous structures.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}

Universal Conceptual Cognitive Annotation \cite[UCCA,][]{abend2013universal}
is a cross-linguistically applicable semantic representation scheme,
building on the established Basic Linguistic Theory typological framework
\cite{Dixon:10b,Dixon:10a,Dixon:12}, and Cognitive
Linguistics literature \cite{croft2004cognitive}.
UCCA is distinct from syntactic
schemes in terms of content and formal structure.
It has demonstrated applicability to multiple languages, including
English, French, German and Czech, support for rapid annotation,
and stability under translation \cite{sulem2015conceptual}.
The scheme has proven useful for machine translation evaluation \cite{birch2016hume}.
Lacking a parser, its applicability has been so far limited,
a gap this work addresses.

We present the first UCCA parser, \parser{}
(Transition-based UCCA Parser),
building on recent advances in discontinuous constituency
and dependency graph parsing, and further introducing novel transitions and features for UCCA.
Transition-based techniques are a natural
starting point for UCCA parsing, given the conceptual similarity of
UCCA's distinctions,
centered around predicate-argument structures, to distinctions expressed
by dependency schemes, on which these methods excel
\cite{dyer2015transition,andor2016globally,kiperwasser2016simple}.
We are further motivated by the strength of transition-based methods
in similar tasks, including dependency graph parsing
\cite{sagae2008shift,ribeyre-villemontedelaclergerie-seddah:2014:SemEval,tokgoz2015transition},
constituency parsing \cite{sagae2005classifier,zhang2009transition,zhu2013fast,maier2015discontinuous,maier-lichte:2016:DiscoNLP},
AMR parsing \cite{wang-xue-pradhan:2015:ACL-IJCNLP,wang2015transition,wang-EtAl:2016:SemEval,dipendra2016neural,goodman2016noise,zhou2016amr,damonte2016incremental}
and CCG parsing \cite{zhang2011shift,ambati2015incremental,ambati-deoskar-steedman:2016:N16-1}.

We evaluate \parser{} on the English UCCA corpora, including an out-of-domain setting.
As a point of comparison, we assess the ability of existing
parsers to tackle the task, by developing a conversion procedure
from UCCA to bilexical graphs and trees.
Results show superior accuracy for \parser{}, demonstrating the effectiveness of
the presented approach.\footnote{Our code will be made freely available upon publication.}

The rest of the paper is structured as follows:
\secref{sec:ucca} describes UCCA in more detail.
\secref{sec:parser} introduces \parser{}.
\secref{sec:exp_setup} discusses the data and experimental setup.
\secref{sec:results} presents the experimental results.
\secref{sec:related_work} summarizes related work, and
\secref{sec:conclusion} concludes the paper.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The UCCA Scheme}\label{sec:ucca}

UCCA graphs are labeled, directed acyclic graphs (DAGs),
whose leaves correspond to the tokens of
the text. A node (or {\it unit}) corresponds to a terminal or
to several sub-units (not necessarily contiguous) viewed as a
single entity according to semantic or cognitive consideration.
Edges bear a category, indicating the role of the sub-unit in the parent relation.
\figref{fig:examples} presents a few examples.

UCCA is a multi-layered representation, where each layer corresponds
to a ``module'' of semantic distinctions.
UCCA's \textit{foundational layer}, targeted in this paper, covers the predicate-argument
structure evoked by predicates of all grammatical categories
(verbal, nominal, adjectival and others), the inter-relations between them,
and other major linguistic phenomena such as coordination and multi-word expressions.
The layer's basic notion is the \textit{scene}, describing a movement, action or state.
Each scene contains one main relation (marked as either a Process or a State),
as well as one or more Participants.
For example, the sentence ``After graduation, John moved to Paris'' (\figref{fig:graduation})
contains two scenes, whose main relations are ``graduation'' and ``moved''.
``John'' is a Participant in both scenes, while ``Paris'' only in the latter.
Further categories account for inter-scene relations and the internal structure of
complex arguments and relations (e.g. coordination, multi-word expressions and modification).

One incoming edge for each non-root node is marked as \textit{primary},
and the rest (mostly used for implicit relations and arguments) as \textit{remote} edges,
a distinction made by the annotator.
The primary edges thus form a tree structure, whereas the remote edges enable reentrancy,
forming a DAG.

\begin{figure}[t]
  \begin{subfigure}{.9\columnwidth}
  \parbox{.05\columnwidth}{\caption{}\label{fig:graduation}}
  \parbox{.8\columnwidth}{
  \scalebox{.9}{
  \begin{tikzpicture}[level distance=10mm, ->,
      every circle node/.append style={fill=black}]
    \node (ROOT) [circle] {}
      child {node (After) {After} edge from parent node[left] {\scriptsize L}}
      child {node (graduation) [circle] {}
      {
        child {node {graduation} edge from parent node[left] {\scriptsize P}}
      } edge from parent node[left] {\scriptsize H} }
      child {node {,} edge from parent node[right] {\scriptsize U}}
      child {node (moved) [circle] {}
      {
        child {node (John) {John} edge from parent node[left] {\scriptsize A}}
        child {node {moved} edge from parent node[left] {\scriptsize P}}
        child {node [circle] {}
        {
          child {node {to} edge from parent node[left] {\scriptsize R}}
          child {node {Paris} edge from parent node[left] {\scriptsize C}}
        } edge from parent node[left] {\scriptsize A} }
      } edge from parent node[right] {\scriptsize H} }
      ;
    \draw[dashed,->] (graduation) to node [auto] {\scriptsize A} (John);
  \end{tikzpicture}
  }}
  \end{subfigure}
  \begin{subfigure}{.9\columnwidth}
  \vspace{-2cm}
  \parbox{.05\columnwidth}{\caption{}\label{fig:gave}}
  \parbox{.75\columnwidth}{\hspace{1cm}
  \scalebox{.9}{
  \begin{tikzpicture}[level distance=12mm, ->,
      every node/.append style={midway},
      every circle node/.append style={fill=black}]
    \node (ROOT) [circle] {}
      child {node {John} edge from parent node[left] {\scriptsize A}}
      child {node [circle] {}
      {
      	child {node {gave} edge from parent node[left] {\scriptsize C}}
      	child {node (everything) {everything} edge from parent[white]}
      	child {node {up} edge from parent node[left] {\scriptsize C}}
      } edge from parent node[right] {\scriptsize P} }
      ;
    \draw[bend right,->] (ROOT) to[out=-20, in=180] node [left] {\scriptsize A} (everything);
  \end{tikzpicture}
  }}
  \parbox{.1\columnwidth}{
  \vspace{2cm}
  \begin{adjustbox}{width=.3\columnwidth,margin=1pt,frame}
  \begin{tabular}{ll}
  P & process \\
  A & participant \\
  H & linked scene \\
  C & center \\
  R & relator \\
  N & connector \\
  L & scene linker \\
  U & punctuation \\
  F & function unit
  \end{tabular}
  \end{adjustbox}
  }
  \end{subfigure}
  \begin{subfigure}{.9\columnwidth}
  \vspace{-1cm}
  \parbox{.05\columnwidth}{\caption{}\label{fig:home}}
  \parbox{.65\columnwidth}{
  \scalebox{.9}{
  \begin{tikzpicture}[level distance=12mm, ->,
      every node/.append style={midway},
      every circle node/.append style={fill=black}]
    \node (ROOT) [circle] {}
      child {node [circle] {}
      {
        child {node {John} edge from parent node[left] {\scriptsize C}}
        child {node {and} edge from parent node[left] {\scriptsize N}}
        child {node {Mary} edge from parent node[left] {\scriptsize C}}
        child {node {'s} edge from parent node[right] {\scriptsize F}}
      } edge from parent node[left] {\scriptsize A} }
      child {node {trip} edge from parent node[left] {\scriptsize P}}
      child {node {home} edge from parent node[left] {\scriptsize A}}
      ;
  \end{tikzpicture}
  }}
  \end{subfigure}
  \caption{\label{fig:examples}
    UCCA structures demonstrating three structural properties exhibited by
    the scheme.
    (\subref{fig:graduation}) includes a remote edge (dashed),
    resulting in ``John'' having two parents.
    (\subref{fig:gave}) includes a discontinuous unit (``gave ... up'').
    (\subref{fig:home}) includes a coordination construction (``John and Mary'').
    Pre-terminal nodes are omitted for brevity.
    Right: legend of edge labels.
  }
\end{figure}

While parsing technology in general, and transition-based parsing in particular,
is well-established for syntactic parsing, UCCA
has several distinct properties
that distinguish it from syntactic representations, mostly in cases where it
abstracts away from syntactic detail that does not
affect meaning.\footnote{As essentially any change in form results
in some change in meaning, it would be more precise to say that
UCCA's foundational layer abstracts away from syntactic variation
not affecting those aspects of meaning which it covers.}
For instance, consider the following examples where the concept of a scene
has a different rationale from the syntactic concept of a clause.
First, non-verbal predicates are represented like verbal ones,
such as when they appear in copula clauses or noun phrases. Indeed,
in \figref{fig:graduation}, ``graduation'' and ``moved'' are considered separate events,
despite appearing in the same clause. 
Second, in the same example, ``John'' is marked as a (remote) Participant
in the graduation scene, despite not being overtly marked.
Third, consider the possessive construction in \figref{fig:home}.
While in UCCA ``trip'' evokes a scene in which ``John and Mary'' is
a Participant, a syntactic scheme would analyze this phrase similarly to ``John and Mary's children''.
As such information is not structurally explicit
(e.g., ``after graduation'' is formally and syntactically very similar to ``after 2pm'',
which does not evoke a scene), semantic parsers are required to produce
different types of generalizations than those required by their syntactic counterparts.
See \secref{sec:related_work} for a discussion of UCCA in the context of other semantic schemes,
such as AMR \cite{banarescu2013abstract}.

%These properties distinguish UCCA from syntactic schemes that already have
%established parsing techonology, and will likely require different techniques.

Alongside recent progress in dependency parsing into projective trees,
there is increasing interest in parsing into 
representations with more general structural properties  (see \secref{sec:related_work}).
One such property is \textit{reentrancy},
referring to semantic units shared between predicates.
For instance, in \figref{fig:graduation},
``John'' is an argument of both ``graduation''
and ``moved'', yielding a DAG rather than a tree.
A second property is \textit{discontinuity},
as in \figref{fig:gave}, where ``gave up'' forms a discontinuous semantic unit.
Discontinuities are pervasive, e.g.,  with multi-word
expressions \cite{schneider2014discriminative}.
Finally, unlike most dependency schemes, UCCA uses \textit{non-terminal nodes}
to represent units comprising more than one word.
The use of non-terminal nodes is motivated by constructions with no clear head, including
coordination structures (e.g., ``John and Mary'' in \figref{fig:home}),
some multi-word expressions (e.g., ``The Haves and the \textit{Have Nots}''),
and prepositional phrases (either the preposition or the head noun can serve as the constituent's head).
To our knowledge, no existing parser supports all structural properties required for UCCA
parsing.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Transition-based UCCA Parsing}\label{sec:parser}

We now turn to presenting \parser{}.
Building on existing knowledge in handling reentrancy, discontinuities and non-terminal nodes,
we define an extended set of transitions and features supporting their conjunction.
Transition-based parsers \cite{Nivre03anefficient} scan the text from start to end,
and create the parse incrementally by applying a \textit{transition}
at each step to the parser state,
defined using three data structures: a buffer $B$ of tokens and nodes to be processed,
a stack $S$ of nodes currently being processed,
and a graph $G=(V,E,\ell)$ of constructed nodes and edges,
where $V$ is the set of \emph{nodes}, $E$ is the set of \emph{edges},
and $\ell : E \to L$ is the \emph{label} function, $L$ being the set of possible labels.
Some states are \textit{terminal}, meaning that $G$ is the final output.
A classifier is used at each step to select the next transition based on features
encoding the parser's current state.
During training, an oracle creates training instances for the classifier,
based on gold-standard annotations.


\begin{figure*}
\begin{adjustbox}{width=\textwidth,margin=3pt,frame}
\begin{tabular}{llll|l|llllc|c}
\multicolumn{4}{c|}{\textbf{\small Before Transition}} & \textbf{\small Transition} & \multicolumn{5}{c|}{\textbf{\small After Transition}} & \textbf{\small Condition} \\
\textbf{\footnotesize Stack} & \textbf{\footnotesize Buffer} & \textbf{\footnotesize Nodes} & \textbf{\footnotesize Edges} & & \textbf{\footnotesize Stack} & \textbf{\footnotesize Buffer} & \textbf{\footnotesize Nodes} & \textbf{\footnotesize Edges} & \textbf{\footnotesize Terminal?} & \\
$S$ & $x \;|\; B$ & $V$ & $E$ & \textsc{Shift} & $S \;|\; x$ & $B$ & $V$ & $E$ & $-$ & \\
$S \;|\; x$ & $B$ & $V$ & $E$ & \textsc{Reduce} & $S$ & $B$ & $V$ & $E$ & $-$ & \\
$S \;|\; x$ & $B$ & $V$ & $E$ & \textsc{Node$_X$} & $S \;|\; x$ & $y \;|\; B$ & $V \cup \{ y \}$ & $E \cup \{ (y,x)_X \}$ & $-$ &
$x \neq \mathrm{root}$ \\
$S \;|\; y,x$ & $B$ & $V$ & $E$ & \textsc{Left-Edge$_X$} & $S \;|\; y,x$ & $B$ & $V$ & $E \cup \{ (x,y)_X \}$ & $-$ &
\multirow{4}{50pt}{\vspace{-5mm}\[\left\{\begin{array}{l}
x \not\in w_{1:n},\\
y \neq \mathrm{root},\\
y \not\leadsto_G x
\end{array}\right.\]} \\
$S \;|\; x,y$ & $B$ & $V$ & $E$ & \textsc{Right-Edge$_X$} & $S \;|\; x,y$ & $B$ & $V$ & $E \cup \{ (x,y)_X \}$ & $-$ & \\
$S \;|\; y,x$ & $B$ & $V$ & $E$ & \textsc{Left-Remote$_X$} & $S \;|\; y,x$ & $B$ & $V$ & $E \cup \{ (x,y)_X^* \}$ & $-$ & \\
$S \;|\; x,y$ & $B$ & $V$ & $E$ & \textsc{Right-Remote$_X$} & $S \;|\; x,y$ & $B$ & $V$ & $E \cup \{ (x,y)_X^* \}$ & $-$ & \\
$S \;|\; x,y$ & $B$ & $V$ & $E$ & \textsc{Swap} & $S \;|\; y$ & $x \;|\; B$ & $V$ & $E$ & $-$ &
$\mathrm{i}(x) < \mathrm{i}(y)$ \\
$[\mathrm{root}]$ & $\emptyset$ & $V$ & $E$ & \textsc{Finish} & $\emptyset$ & $\emptyset$ & $V$ & $E$ & $+$ & \\
\end{tabular}
\end{adjustbox}
\caption{\label{fig:transitions}
  The transition set of \parser{}. %Following standard practice,
  We write the stack with its top to the right and the buffer with its head to the left.
  $(\cdot,\cdot)_X$ denotes a primary $X$-labeled edge, and $(\cdot,\cdot)_X^*$ a remote $X$-labeled edge.
  $\mathrm{i}(x)$ is a running index for the created nodes.
  In addition to the specified conditions,
  the prospective child in an \textsc{Edge} transition must not already have a primary parent.
}
\end{figure*}

\paragraph{Transition Set.}
Given a sequence of tokens $w_1, \ldots, w_n$, we predict a UCCA graph $G$ over the sequence.
Parsing starts with a single node on the stack (an artificial root node), and the input tokens
in the buffer. The set of transitions is given in \figref{fig:transitions}.
In addition to the standard \textsc{Shift} and \textsc{Reduce} operations, 
we follow previous work in transition-based constituency parsing \cite{sagae2005classifier},
adding the \textsc{Node} transition for creating new non-terminal nodes.
\textsc{Node$_X$} creates a new node on the buffer as a parent of the first element on the stack, with an $X$-labeled edge.

\textsc{Left-Edge$_X$} and \textsc{Right-Edge$_X$} create a new primary $X$-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively.
As a UCCA node may only have one incoming primary edge,
\textsc{Edge} transitions are disallowed if the child node already
has an incoming primary edge.
\textsc{Left-Remote$_X$} and \textsc{Right-Remote$_X$} do not have this restriction,
and the created edge is additionally marked as \textit{remote}.
We distinguish between these two pairs of transitions to allow the parser to create remote edges
without the possibility of producing invalid graphs.
To support the prediction of multiple parents, node and edge transitions
leave the stack unchanged, as in other work on
transition-based dependency graph parsing
\cite{sagae2008shift,ribeyre-villemontedelaclergerie-seddah:2014:SemEval,tokgoz2015transition}.
\textsc{Reduce} pops the stack, to allow removing a node
once all its edges have been created.
To handle discontinuous nodes, \textsc{Swap} pops the second
node on the stack and adds it to the top of the buffer, as with the similarly
named transition in previous work \cite{nivre2009non,maier2015discontinuous}.
Finally, \textsc{Finish} pops the root node and marks the state as terminal.

\paragraph{Classifier.}
The choice of classifier and feature representation has been shown to play an important role in
transition-based parsing \cite{chen2014fast,andor2016globally,kiperwasser2016simple}.
To investigate the impact of the type of transition classifier in UCCA parsing,
we experiment with three different models.
\begin{enumerate}[leftmargin=*]
\item
Starting with a simple and common choice \cite[e.g.,][]{maier-lichte:2016:DiscoNLP},
\textbf{\parser{Sparse}} uses a linear classifier with sparse features, trained with
the averaged structured perceptron algorithm
\cite{Coll:04} and \textsc{MinUpdate} \cite{goldberg2011learning}:
each feature requires a minimum number of updates in training
to be included in the model.\footnote{We also experimented with a linear model using
dense embedding features, but still trained with averaged structured perceptron.
It performed worse than the sparse perceptron model and was hence discarded.}
\item
Changing the model to a feedforward neural network with dense embedding features,
\textbf{\parser{MLP}}
(``multi-layer perceptron''), uses an architecture similar to that of \citet{chen2014fast},
but with two rectified linear layers
and dropout \cite{srivastava2014dropout}.
The embeddings are trained jointly with the classifier.
\item
Finally, \textbf{\parser{BiLSTM}} uses a bidirectional LSTM for feature representation,
on top of the dense embedding features,
using an architecture similar to \citet{kiperwasser2016simple}.
The BiLSTM runs on the input tokens in both forward and backward directions,
yielding a vector representation that is then concatenated with dense features representing the
parser state (e.g., existing edge labels and previous parser actions).
This representation is then fed into a feedforward network similar to \parser{MLP}.
The feedforward layers, BiLSTM and embeddings are all trained jointly.
\end{enumerate}

For all classifiers, inference is performed greedily, i.e., without beam search.
Hyperparameters are tuned on the development set (see \secref{sec:exp_setup}).

\begin{figure}[t]
	\begin{tikzpicture}[level distance=8mm, sibling distance=1cm]
	\node[anchor=west] at (0,1.5) {Parser state};
	\draw[color=gray,dashed] (0,-1.2) rectangle (7.5,1.25);
	\draw[color=gray] (.1,0) rectangle (1.5,.5);
	\node[anchor=west] at (.1,.8) {$S$};
	\node[fill=black, circle] at (.4,.275) {};
	\node[fill=blue, circle] at (.9,.275) {};
	\node[anchor=west] at (1.15,.175) {\small ,};
	\draw[color=gray] (1.95,0) rectangle (4.9,.5);
	\node[anchor=west] at (1.95,.8) {$B$};
	\node[anchor=west] at (1.95,.275) {\small John moved to Paris .};
	\node[anchor=west] at (5.1,.8) {$G$};
	\node[fill=black, circle] at (6.35,.75) {}
	  child {node  {\scriptsize After} edge from parent [->] node[left] {\small L}}
	  child {node [fill=blue, circle] {}
	  {
	    child {node {\scriptsize graduation} edge from parent [->] node[right] {\small P}}
	  } edge from parent [->] node[right] {\small H} };
	\end{tikzpicture}
	\begin{tikzpicture}[->]
	\node[anchor=west] at (0,6) {Transition classifier};
	\tiny
	\tikzstyle{main}=[circle, minimum size=7mm, draw=black!80, node distance=12mm]
	\foreach \i/\word in {1/{After},3/{graduation},5/{to},7/{Paris}} {
	    \node (x\i) at (\i,-1.3) {\word};
	    \node[main, fill=white!100] (h\i) at (\i,0) {LSTM};
        \path (x\i) edge (h\i);
	    \node[main, fill=white!100] (i\i) at (\i.5,.8) {LSTM};
        \path (x\i) edge [bend right] (i\i);
	    \node[main, fill=white!100] (l\i) at (\i.5,2.3) {LSTM};
        \path (h\i) edge [bend left] (l\i);
        \path (i\i) edge (l\i);
	    \node[main, fill=white!100] (k\i) at (\i,3.1) {LSTM};
        \path (i\i) edge [bend left] (k\i);
        \path (h\i) edge [bend left] (k\i);
	}
    \node (l4) at (4.5,2.3) {\ldots};
    \node (k4) at (4,3.1) {\ldots};
    \node (i4) at (4.5,.8) {\ldots};
    \node (h4) at (4,0) {\ldots};
    \node (x4) at (4,-1.3) {\ldots};
	\foreach \current/\next in {1/3,3/4,4/5,5/7} {
        \path (i\next) edge (i\current);
        \path (h\current) edge (h\next);
        \path (k\next) edge (k\current);
        \path (l\current) edge (l\next);
	}
    \node[main, fill=white!100] (mlp) at (4,4.6) {MLP};
	\foreach \i in {1,3} {
        \path (l\i) edge (mlp);
        \path (k\i) edge (mlp);
    }
    \coordinate (state) at (6.5,6.5);
    \path (state) edge [bend left] (mlp);
    \node (transition) at (4,5.8) {\textsc{Node}\textsubscript{U}};
    \path (mlp) edge (transition);
	\end{tikzpicture}
	\caption{Illustration of the parser model.
	Top: parser state (stack, buffer and intermediate graph).
	Bottom: \parser{BiLTSM} architecture.
	Vector representation for the input tokens is computed
	by two layers of bidirectional LSTMs.
	The vectors for specific tokens is concatenated with
	embedding and numeric features from the parser state
	(for existing edge labels, number of children, etc.),
	and fed into the MLP for selecting the next transition.}
	\label{fig:model}
\end{figure}

\paragraph{Features.}
\parser{Sparse} uses binary indicator features representing
the words, POS tags, syntactic dependency labels and
existing edge labels related to the top four stack elements and the 
next three buffer elements, in addition to their children and grandchildren in the graph.
We also use bi- and trigram features based on these values \cite{zhang2009transition,zhu2013fast},
features related to discontinuous nodes
\cite[including separating punctuation and gap type]{maier2015discontinuous},
features representing existing edges and the number of parents and children a node has,
and the past actions taken by the parser.
We also use a novel, UCCA-specific feature:
number of remote children.\footnote{See
Appendix~\ref{appendix:features} for a full list of used feature templates.}

For \parser{MLP} and \parser{BiLSTM},
we replace all indicator features by a
concatenation of the vector embeddings of all represented elements:
words, POS tags, syntactic dependency labels, edge labels, punctuation, gap type and parser actions.
These embeddings are initialized randomly.
We additionally use external word embeddings initialized with
pre-trained word2vec vectors \cite{mikolov2013efficient},\footnote{\url{
https://goo.gl/6ovEhC}} updated during training.
In addition to dropout between NN layers, we apply word dropout 
\cite{kiperwasser2016simple}: with a certain probability, the embedding for a
word is replaced with a zero vector. We do not apply word dropout to the external
word embeddings.

Finally, for all classifiers we add another real-valued feature to the input vector,
\textbf{ratio}, corresponding to the ratio between the number of terminals to number of nodes
in the graph $G$.
This novel feature serves as a regularizer for the creation of new nodes,
and should be beneficial for other transition-based constituency parsers too.

\paragraph{Training.}
For training the transition classifiers, we use a dynamic oracle \cite{goldberg2012dynamic},
i.e., an oracle that outputs a set of optimal transitions: when
applied to the current parser state, the gold
standard graph is reachable from the resulting state.
For example, the oracle would predict a \textsc{Node} transition if the stack 
has on its top a parent in the gold graph that has not been created,
but would predict a \textsc{Right-Edge} transition if the second stack
element is a parent of the
first element according to the gold graph and the edge between them has not been created.
The transition predicted by the classifier is deemed correct
and is applied to the parser state to reach the subsequent state,
if the transition is included in the set of optimal transitions.
Otherwise, a random optimal transition is applied,
and for the perceptron-based parser, the classifier's weights are updated according
to the perceptron update rule.

POS tags and syntactic dependency labels are extracted using spaCy
\cite{honnibal-johnson:2015:EMNLP}.\footnote{\url{https://spacy.io}}
We use the categorical cross-entropy objective function and optimize the
NN classifiers with the Adam optimizer \cite{kingma2014adam}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Setup}\label{sec:exp_setup}

\paragraph{Data.}
We conduct our main experiments on the UCCA Wikipedia corpus (henceforth, \textit{Wiki}),
and use the English part of the UCCA \textit{Twenty Thousand Leagues Under the Sea}
English-French parallel corpus (henceforth, \textit{20K Leagues}) as
out-of-domain data.\footnote{\mbox{\url{http://cs.huji.ac.il/~oabend/ucca.html}}}
\tabref{table:data} presents some statistics for the two corpora.
We use passages of indices up to 676
of the \textit{Wiki} corpus as our training set, passages 688--808 as development set,
and passages 942--1028 as in-domain test set.
While UCCA edges can cross sentence boundaries, we adhere to the common
practice in semantic parsing and train our parsers on individual sentences,
discarding inter-relations between them (0.18\% of the edges).
We also discard linkage nodes and edges (as they often express inter-sentence
relations and are thus mostly redundant when applied at the sentence level)
as well as implicit nodes.\footnote{Appendix~\ref{appendix:extended_ucca}
further discusses linkage and implicit units.}
In the out-of-domain experiments, we apply the same parsers
(trained on the \textit{Wiki} training set) to the \textit{20K Leagues} corpus
without parameter re-tuning.


\begin{table}
\scalebox{.9}{
\begin{tabular}{l|ccc|c}
& \multicolumn{3}{c|}{Wiki} & 20K \\
& \small Train & \small Dev & \small Test & Leagues \\
\hline
\# passages & 300 & 34 & 33 & 154 \\
\# sentences & 4268 & 454 & 503 & 506 \\
\hline
\# nodes & 298,993 & 33,704 & 35,718 & 29,315 \\
\% terminal & 42.96 & 43.54 & 42.87 & 42.09 \\
\% non-term. & 58.33 & 57.60 & 58.35 & 60.01 \\
\% discont. & 0.54 & 0.53 & 0.44 & 0.81 \\
\% reentrant & 2.38 & 1.88 & 2.15 & 2.03 \\
\hline
\# edges & 287,914 & 32,460 & 34,336 & 27,749 \\
\% primary & 98.25 & 98.75 & 98.74 & 97.73 \\
\% remote & 1.75 & 1.25 & 1.26 & 2.27 \\
\hline
\multicolumn{3}{l}{\footnotesize Average per non-terminal node} \\
\# children & 1.67 & 1.68 & 1.66 & 1.61 
\end{tabular}
}
\caption{Statistics of the \textit{Wiki} and \textit{20K Leagues} UCCA corpora.
All counts exclude the root node, implicit nodes, and linkage nodes and edges.
}
\label{table:data}
\end{table}

\paragraph{Implementation.}
We use the DyNet package \cite{neubig2017dynet} for implementing the NN classifiers.
Unless otherwise noted, we use the default values provided by the package.
See Appendix~\ref{appendix:hyperparameters} for the hyperparameter values we found by tuning
on the development set.

\paragraph{Evaluation.}
We define a simple measure for comparing UCCA structures
$G_p=(V_p,E_p,\ell_p)$ and $G_g=(V_g,E_g,\ell_g)$,
the predicted and gold-standard graphs, respectively, over the same
sequence of terminals $W = \{w_1,\ldots,w_n\}$.
For an edge $e=(u,v)$ in either graph,
$u$ being the parent and $v$ the child, its yield $y(e) \subseteq W$ is the
set of terminals in $W$ that are descendants of $v$.
Define the set of \textit{mutual edges} between $G_p$ and $G_g$:

\vspace{-.6cm}

{\small
\begin{multline*}
    M(G_p,G_g) = \\
    \left\{(e_1,e_2) \in E_p \times E_g \;|\;
    y(e_1) = y(e_2) \wedge \ell_p(e_1)=\ell_g(e_2)\right\}
\end{multline*}
}

\vspace{-.6cm}

Labeled precision and recall are defined by dividing $|M(G_p,G_g)|$ by $|E_p|$ and $|E_g|$, respectively,
and F-score by taking their harmonic mean.
We report two variants of this measure: one where we consider only primary edges,
and another for remote edges (see \secref{sec:ucca}).
Performance on remote edges is of pivotal importance in this investigation,
which focuses on extending the class of graphs supported by statistical parsers.

We note that the measure collapses to the standard
PARSEVAL constituency evaluation measure if $G_p$ and $G_g$ are trees.
Punctuation is excluded from the evaluation, but not from the datasets.

\begin{figure}
\centering
\scalebox{.9}{
\begin{dependency}[theme = simple]
\begin{deptext}[column sep=.7em,ampersand replacement=\^]
After \^ graduation \^ , \^ John \^ moved \^ to \^ Paris \\
\end{deptext}
\depedge{2}{1}{L}
\depedge{2}{3}{U}
\depedge[dashed]{2}{4}{A}
\depedge{5}{4}{A}
\depedge{2}{5}{H}
\depedge{7}{6}{R}
\depedge{5}{7}{A}
\end{dependency}
}
\scalebox{.9}{
\begin{dependency}[theme = simple]
\begin{deptext}[column sep=.7em,ampersand replacement=\^]
John \^ gave \^ everything \^ up \\
\end{deptext}
\depedge{1}{2}{A}
\depedge{3}{2}{A}
\depedge{4}{2}{C}
\end{dependency}
}
\scalebox{.9}{
\begin{dependency}[theme = simple]
\begin{deptext}[column sep=.7em,ampersand replacement=\^]
John \^ and \^ Mary \^ went \^ home \\
\end{deptext}
\depedge[edge start x offset=-6pt]{1}{4}{A}
\depedge{2}{1}{N}
\depedge{3}{1}{C}
\depedge{5}{4}{A}
\end{dependency}
}
\caption{Bilexical approximation for sentences in \figref{fig:examples}.}
\label{fig:bilexical_example}
\end{figure}

\begin{table*}
\begin{tabular}{l|ccc|ccc||ccc|ccc}
& \multicolumn{6}{c||}{Wiki (in-domain)} & \multicolumn{6}{c}{20K Leagues (out-of-domain)} \\
& \multicolumn{3}{c|}{Primary} & \multicolumn{3}{c||}{Remote}
& \multicolumn{3}{c|}{Primary} & \multicolumn{3}{c}{Remote} \\
& \textbf{LP} & \textbf{LR} & \textbf{LF} & \textbf{LP} & \textbf{LR} & \textbf{LF}
& \textbf{LP} & \textbf{LR} & \textbf{LF} & \textbf{LP} & \textbf{LR} & \textbf{LF} \\
\hline
\parser{Sparse}
& 64.5 & 63.7 & 64.1 & 19.8 & 13.4 & 16
& 59.6 & 59.9 & 59.8 & 22.2 & 7.7 & 11.5 \\
%\parser{Dense} 
%& 59.1 & 58.9 & 59 & 17.4 & 12.4 & 14.5
%& 57.0 & 57.9 & 57.4 & 10.8 & 4.2 & 6.0 \\
\parser{MLP}
& 65.2 & 64.6 & 64.9 & 23.7 & 13.2 & 16.9
& 62.3 & 62.6 & 62.5 & 20.9 & 6.3 & 9.7 \\
\parser{BiLSTM}
& 74.4 & 72.7 & \textbf{73.5} & 47.4 & 51.6 & \textbf{49.4}
& 68.7 & 68.5 & \textbf{68.6} & 38.6 & 18.8 & \textbf{25.3} \\
\hline
\multicolumn{8}{l}{\rule{0pt}{2ex} \footnotesize
Bilexical Approximation (Dependency DAG Parsers)} \\
\small Upper Bound
%& \small 94.5 & \small 87.7 & \small 91 & \small 77.3 & \small 46.8 & \small 58.3
%& \small 94.8 & \small 88 & \small 91.3 & \small 66.3 & \small 32.3 & \small 43.4 \\
& & & \small 91 & & & \small 58.3
& & & \small 91.3 & & & \small 43.4 \\
DAGParser
& 61.8 & 55.8 & 58.6 & 9.5 & 0.5 & 1
& 56.4 & 50.6 & 53.4 & -- & 0 & 0 \\
TurboParser
& 57.7 & 46 & 51.2 & 77.8 & 1.8 & 3.7
& 50.3 & 37.7 & 43.1 & 100 & 0.4 & 0.8 \\
\hline
\multicolumn{8}{l}{\rule{0pt}{2ex} \footnotesize
Tree Approximation (Constituency Tree Parser)} \\
\small Upper Bound
%& \small 100 & \small 100 & \small 100 & & &
%& \small 100 & \small 100 & \small 100 \\
& & & \small 100 & & &
& & & \small 100 \\
\textsc{uparse}
& 60.9 & 61.2 & 61.1 & & &
& 52.7 & 52.8 & 52.8 \\
\hline
\multicolumn{8}{l}{\rule{0pt}{2ex} \footnotesize
Bilexical Tree Approximation (Dependency Tree Parsers)} \\
\small Upper Bound
%& \small 94.5 & \small 87.7 & \small 91 & & &
%& \small 94.8 & \small 88 & \small 91.3 \\
& & & \small 91 & & &
& & & \small 91.3 \\
MaltParser
& 62.8 & 57.7 & 60.2 & & &
& 57.8 & 53 & 55.3 \\
LSTM Parser
& 73.2 & 66.9 & 69.9 & & &
& 66.1 & 61.1 & 63.5
\end{tabular}
\caption{
  Experimental results, in percents, on the \textit{Wiki} test set (left)
  and the \textit{20K Leagues} set (right).
  Columns correspond to labeled precision, recall and F-score,
  for both primary and remote edges.
  \parser{BiLSTM} obtains the highest F-scores in all metrics, surpassing the
  bilexical parsers, tree parsers and other classifiers.
}
\label{table:results}
\end{table*}

\paragraph{Comparison to bilexical graph parsers.}
As no direct comparison with existing parsers is possible,
we compare \parser{} to bilexical dependency graph parsers,
which support reentrancy and discontinuity but not non-terminal nodes.
To facilitate the comparison, we convert our training set into bilexical graphs
(see examples in \figref{fig:bilexical_example}),
train each of the parsers,
and evaluate them by applying them to the test set
and then reconstructing UCCA graphs, which are compared with the gold standard.\footnote{See
Appendix~\ref{appendix:conversion} for the conversion procedures.}
In \secref{sec:results} we report the upper bounds on the achievable scores due to the error resulting from the removal of non-terminal nodes.

\begin{figure}
\centering
\scalebox{.9}{
  \begin{tikzpicture}[level distance=10mm, ->]
    \node (ROOT) [fill=black, circle] {}
      child {node (After) {After} edge from parent node[left] {\scriptsize $L$}}
      child {node (graduation) [fill=black, circle] {}
      {
        child {node {graduation} edge from parent node[left] {\scriptsize $P$}}
      } edge from parent node[left] {\scriptsize $H$} }
      child {node {,} edge from parent node[right] {\scriptsize $U$}}
      child {node (moved) [fill=black, circle] {}
      {
        child {node (John) {John} edge from parent node[left] {\scriptsize $A$}}
        child {node {moved} edge from parent node[left] {\scriptsize $P$}}
        child {node [fill=black, circle] {}
        {
          child {node {to} edge from parent node[left] {\scriptsize $R$}}
          child {node {Paris} edge from parent node[left] {\scriptsize $C$}}
        } edge from parent node[left] {\scriptsize $A$} }
      } edge from parent node[right] {\scriptsize $H$} }
      ;
  \end{tikzpicture}
}
\scalebox{.9}{
	\begin{dependency}[theme = simple]
	\begin{deptext}[column sep=.7em,ampersand replacement=\^]
	After \^ graduation \^ , \^ John \^ moved \^ to \^ Paris \\
	\end{deptext}
	\depedge{2}{1}{L}
	\depedge{2}{3}{U}
	\depedge{5}{4}{A}
	\depedge{2}{5}{H}
	\depedge{7}{6}{R}
	\depedge{5}{7}{A}
	\end{dependency}
  }
  \caption{Tree approximation for the sentence in \figref{fig:graduation} (top),
  and bilexical tree approximation for the same sentence (bottom).
  These are identical to the original graphs,
  apart from the removal of remote edges.}
  \label{fig:tree_example}
\end{figure}

\paragraph{Comparison to tree parsers.}
For completeness,
we also perform a \textit{tree approximation} experiment, converting UCCA to (bilexical) trees
and evaluating constituency and dependency tree parsers on them
(see examples in \figref{fig:tree_example}).
Our approach is similar
to the tree approximation approach used for dependency graph parsing
\cite{agic2015semantic,fernandez2015parsing},
where dependency graphs were converted into dependency trees
and then parsed by dependency tree parsers.
In our setting, the conversion to trees consists simply of removing remote edges from the 
graph, and then to bilexical trees by applying the same procedure as for bilexical graphs.

\paragraph{Baseline parsers.}
We evaluate two bilexical graph semantic dependency parsers:
DAGParser \cite{ribeyre-villemontedelaclergerie-seddah:2014:SemEval}, the leading 
transition-based parser in SemEval 2014 \cite{oepen2014semeval} and
TurboParser \cite{almeida-martins:2015:SemEval},
a graph-based parser from SemEval 2015 
\cite{oepen2015semeval};
\textsc{uparse} \cite{maier-lichte:2016:DiscoNLP},
a transition-based constituency parser supporting discontinuous constituents;
and two bilexical tree parsers:
MaltParser \cite{nivre2007maltparser},\footnote{For
MaltParser we use the \textsc{ArcEager} transition set and SVM classifier.
Other configurations yielded lower scores.}
and the stack LSTM-based parser of
\citet[henceforce ``LSTM Parser'']{dyer2015transition}.
Default settings are used in all cases.
DAGParser and \textsc{uparse} use beam search by default, with a beam size of 5 and 4
respectively. The other parsers are greedy.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}\label{sec:results}

\tabref{table:results} presents our main experimental results, as well as
upper bounds for the baseline parsers,
reflecting the error resulting from the conversion.

DAGParser and \textsc{uparse} are most directly comparable to
\parser{Sparse}, as they also use a perceptron classifier with sparse features.
\parser{Sparse} considerably outperforms both, where
DAGParser does not predict any remote edges in the out-of-domain setting.
TurboParser fares worse in this comparison, despite somewhat better results on
remote edges.
The LSTM parser of \citet{dyer2015transition} obtains the highest primary F-score
among the baseline parsers, with a considerable margin.
It obtains 9.7\% F-score higher than MaltParser,
despite being limited by the same approximation error upper bound,
and using a similar transition set.
The performance difference between them should therefore be attributed to the classification model.

Using a feedforward NN and embedding features,
\parser{MLP} obtains higher scores than \parser{Sparse},
but is outperformed by the LSTM parser on primary edges.
However, using better input encoding,
\parser{BiLSTM} obtains substantially higher scores than \parser{MLP}
and all other parsers, on both primary and remote edges,
both in the in-domain and out-of-domain settings.
Its performance in absolute terms, of 73.5\% F-score on primary edges,
is encouraging in light of
UCCA's inter-annotator agreement of 80--85\%
F-score on them \cite{abend2013universal}.

The parsers resulting from tree approximation are unable to recover any remote edges,
as these are removed in the conversion.\footnote{We
also experimented with a simpler version of \parser{} lacking
\textsc{Remote} transitions, obtaining an increase of up to 2 labeled F-score
points on primary edges, at the cost of not being able to predict remote edges.}
The bilexical DAG parsers are also quite limited in this respect,
due to a conversion error upper bound of 58.3\%.
However, this upper bound cannot in itself account for the bilexical DAG parsers'
poor performance on remote edges, which is an order of magnitude lower than that
of \parser{BiLSTM}.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}\label{sec:related_work}

While earlier work on anchored\footnote{By {\it anchored} we mean that the semantic representation
directly corresponds to the words and phrases of the text.}
semantic parsing has mostly concentrated on shallow semantic analysis,
focusing on semantic role labeling of verbal argument structures,
the focus has recently shifted to parsing of more elaborate representations that account
for a wider range of phenomena.

\paragraph{Grammar-Based Parsing.}
Linguistically expressive grammars such as HPSG \cite{PandS:94}, CCG \cite{Steedman:00} and TAG \cite{Joshi:97}
provide a theory of the syntax-semantics interface, and have been used as a basis for semantic parsers
by defining compositional semantics on top of them \cite[among others]{Flic:00,bos2005towards}.
Depending on the grammar and the implementation, such semantic parsers can support
some or all of the structural properties UCCA supports.
Nevertheless, this line of work differs from our grammarless approach in two important ways.
First, the semantic representations are different. UCCA does not attempt to model
the syntax-semantics interface and is thus less coupled with syntax in comparison to
compositional semantic structures.
Second, while grammar-based parsers explicitly model syntax, grammarless
approaches, such as the ones presented here, seek to directly model the relation between
strings and semantic structures.

\paragraph{Broad-Coverage Semantic Parsing.}
Most closely related to this work is Broad-Coverage Semantic Dependency Parsing (SDP),
addressed in two SemEval tasks \cite{oepen2014semeval,oepen2015semeval}.
Like UCCA parsing, SDP addresses a wide range of semantic phenomena,
and supports discontinuous units and reentrancy.
However, SDP uses bilexical dependencies, disallowing non-terminal nodes, which
are useful for representing structures that have no clear head, such as coordination
\cite[see \secref{sec:ucca}]{Ivanova2012who}. It also differs from UCCA in the type
of distinctions it makes, which are more closely related to the syntax-semantics interface,
where UCCA aims to capture purely semantic cross-linguistically applicable notions, abstracting
away, as much as possible, from syntactic distinctions.\oa{I need to sharpen this}
The target representations comprising SDP are formed on the basis of functional syntactic
analysis, with the addition of semantic roles and other relations.
However, in comparison to UCCA they remain syntactic in nature.
For instance, the ``poss'' label in the DM target representation is used to
annotate syntactic possessive constructions, regardless of whether they correspond to
semantic possession or denote a different meaning, such as evoking a predicate.
Recent interest in SDP has yielded numerable works on graph parsing
\cite{ribeyre-villemontedelaclergerie-seddah:2014:SemEval,thomson-EtAl:2014:SemEval,almeida-martins:2015:SemEval,du-EtAl:2015:SemEval}, including work on graph parsing
by tree approximation \cite{agic-koller:2014:SemEval,schluter-EtAl:2014:SemEval}
and on joint syntactic/semantic parsing
\cite{henderson2013multilingual,swayamdipta-EtAl:2016:CoNLL}.

\paragraph{Abstract Meaning Representation.}
Another line of work addresses parsing into AMRs
\cite{flanigan2014discriminative,vanderwende2015amr,pust2015parsing,artzi2015broad},
which, like UCCA, abstract away from syntactic distinctions
and represent meaning directly, using OntoNotes predicates.
Events in AMR may also be evoked by non-verbal predicates, including possessive constructions.
Unlike in UCCA, in AMR, the alignment between concepts and the text is not explicitly marked.
While sharing much of this work's motivation, not anchoring the representation in the text
complicates the parsing task, as it requires
the alignment to be automatically (and imprecisely) detected.
Indeed, despite considerable technical effort
\cite{flanigan2014discriminative,pourdamghani2014aligning,werling2015robust},
concept identification is only about 80\%--90\% accurate.
Furthermore, anchoring allows breaking down sentences into semantically meaningful sub-spans,
which is useful for many applications \cite{fernandez2015parsing,birch2016hume}.
CAMR \cite{wang-xue-pradhan:2015:ACL-IJCNLP,wang2015transition,wang-EtAl:2016:SemEval,goodman2016noise}
is a transition-based AMR parser that operates by first syntactically parsing the input,
and then processing the result into AMR.
In contrast,
\citet{damonte2016incremental} developed a transition-based AMR parser that
operates on the input tokens, but still requires concept identification, using
a simple heuristic selecting the most frequent graph for each token.
The transition-based AMR parser by \citet{zhou2016amr} performs concept identification
and parsing jointly.
\parser{} does not require separately aligning the input tokens to the graph,
processing them directly instead.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{sec:conclusion}

We present \parser{}, the first parser for UCCA.
Evaluated in in-domain and out-of-domain settings, we show that coupled with a
NN classifier and BiLSTM feature extractor,
it accurately predicts UCCA graphs from text, outperforming a variety of
strong baselines by a margin.
While much recent work focused on semantic parsing of different types,
the effectiveness of different parsing approaches for schemes with
different structural and semantic properties is not well-understood
\cite{kuhlmann2016towards}.
We make a contribution to this literature by proposing a general grammarless parser
that supports multiple parents, discontinuous units and non-terminal nodes.

Future work will explore different target
representations and more sophisticated conversion procedures \cite{kong-15},
to shed light on the commonalities and differences between
representations, suggesting ways for a data-driven design of semantic annotation.
A parser for UCCA will enable using the framework for new tasks,
in addition to existing applications such as machine translation
evaluation \cite{birch2016hume}.
We believe UCCA's merits in providing a cross-linguistically applicable,
broad-coverage annotation will support ongoing efforts to incorporate deeper
semantic structures into various applications,
such as sentence simplification \cite{narayan2014hybrid} and summarization \cite{liu2015toward}.


\newpage
\bibliography{references}
\bibliographystyle{acl_natbib}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\section{Feature Templates}
\label{appendix:features}

\figref{fig:features} presents the feature templates used by \parser{Sparse}.
All feature templates define binary features.
The other classifiers use the same elements listed in the feature templates,
but all categorical features are replaced by vector embeddings,
and all count-based features are replaced by their numeric value.

For some of the features, we used the notion of \textit{head word},
defined by the $h^*$ function (see Appendix~\ref{appendix:conversion}).
While head words are not explicitly represented in the UCCA scheme, these
features prove useful as means of encoding word-to-word relations.

\begin{figure*}
\centering
\begin{adjustbox}{margin=3pt,frame}
\begin{tabular}{l}
{\footnotesize Features from \cite{zhang2009transition}:} \\
\textbf{unigrams} \\
$s_0tde, s_0we, s_1tde, s_1we, s_2tde, s_2we, s_3tde, s_3we,$ \\
$b_0wtd, b_1wtd, b_2wtd, b_3wtd,$ \\
$s_0lwe, s_0rwe, s_0uwe, s_1lwe, s_1rwe, s_1uwe$ \\
\textbf{bigrams} \\
$s_0ws_1w, s_0ws_1e, s_0es_1w, s_0es_1e, s_0wb_0w, s_0wb_0td,$ \\
$s_0eb_0w, s_0eb_0td, s_1wb_0w, s_1wb_0td, s_1eb_0w, s_1eb_0td,$ \\
$b_0wb_1w, b_0wb_1td, b_0tdb_1w, b_0tdb_1td$ \\
\textbf{trigrams} \\
$s_0es_1es_2w, s_0es_1es_2e, s_0es_1eb_0w, s_0es_1eb_0td,$ \\
$s_0es_1wb_0w, s_0es_1wb_0td, s_0ws_1es_2e, s_0ws_1eb_0td$ \\
\textbf{separator} \\
$s_0wp, s_0wep, s_0wq, s_0wcq, s_0es_1ep, s_0es_1eq,$ \\
$s_1wp, s_1wep, s_1wq, s_1weq$ \\

\textbf{extended} \footnotesize \cite{zhu2013fast} \\
$s_0llwe, s_0lrwe, s_0luwe, s_0rlwe, s_0rrwe,$ \\
$s_0ruwe, s_0ulwe, s_0urwe, s_0uuwe, s_1llwe,$ \\
$s_1lrwe, s_1luwe, s_1rlwe, s_1rrwe, s_1ruwe$ \\
\end{tabular}
\begin{tabular}{l}
\textbf{disco} \footnotesize \cite{maier2015discontinuous} \\
$s_0xwe, s_1xwe, s_2xwe, s_3xwe,$ \\
$s_0xtde, s_1xtde, s_2xtde, s_3xtde,$ \\
$s_0xy, s_1xy, s_2xy, s_3xy$ \\
$s_0xs_1e, s_0xs_1w, s_0xs_1x, s_0ws_1x, s_0es_1x,$ \\
$s_0xs_2e, s_0xs_2w, s_0xs_2x, s_0ws_2x, s_0es_2x,$ \\
$s_0ys_1y, s_0ys_2y, s_0xb_0td, s_0xb_0w$ \\

{\footnotesize Features from \cite{tokgoz2015transition}:} \\
\textbf{counts} \\
$s_0P, s_0C, s_0wP, s_0wC, b_0P, b_0C, b_0wP, b_0wC$ \\
\textbf{edges} \\
$s_0s_1, s_1s_0, s_0b_0, b_0s_0, s_0b_0e, b_0s_0e$ \\
\textbf{history} \\
$a_0, a_1$ \\

\textbf{remote} \footnotesize (Novel, UCCA-specific features) \\
$s_0R, s_0wR, b_0R, b_0wR$
\end{tabular}
\end{adjustbox}
\captionsetup{singlelinecheck=off}
\caption[]{\label{fig:features}
  Binary feature templates for \parser{Sparse}. Notation:\\
  $s_i$, $b_i$: $i$th stack and buffer items.\\
  $w$, $t$, $d$: word form, POS tag and syntactic dependency label of the terminal returned by $h^*(\cdot)$
  (see Appendix~\ref{appendix:conversion}).\\
  $e$: edge label to the node returned by $h(\cdot)$.\\
  $l$, $r$ ($ll$, $rr$): leftmost and rightmost (grand)children.\\
  $u$ ($uu$): unary (grand)child, when only one exists.\\
  $p$: unique separator punctuation between $s_0$ and $s_1$. $q$: separator count.\\
  $x$: gap type (``none'', ``pass'' or ``gap'') at the sub-graph under the current node.\\
  $y$: sum of gap lengths \protect\cite{Maier:Lichte:11}.\\
  $P$, $C$: number of parents and children.\\
  $R$: number of remote children.\\
  $a_i$: action taken $i$ steps back.
}
\end{figure*}

\section{Extended Presentation of UCCA}
\label{appendix:extended_ucca}

This work does not handle two important constructions in the UCCA foundational layer:
Linkage, representing discourse relations, and Implicit, representing covert entities.
\tabref{table:data_linkage_implicit} shows the statistics of linkage nodes and edges and
implicit nodes in the corpora.

\paragraph{Linkage.}

\figref{fig:example_linkage} demonstrates a linkage relation, omitted from \figref{fig:graduation}.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[level distance=10mm, ->]
    \node (ROOT) [fill=black, circle] {}
      child {node (After) {After} edge from parent node[left] {\scriptsize $L$}}
      child {node (graduation) [fill=black, circle] {}
      {
        child {node {graduation} edge from parent node[left] {\scriptsize $P$}}
      } edge from parent node[left] {\scriptsize $H$} }
      child {node {,} edge from parent node[right] {\scriptsize $U$}}
      child {node (moved) [fill=black, circle] {}
      {
        child {node (John) {John} edge from parent node[left] {\scriptsize $A$}}
        child {node {moved} edge from parent node[left] {\scriptsize $P$}}
        child {node [fill=black, circle] {}
        {
          child {node {to} edge from parent node[left] {\scriptsize $R$}}
          child {node {Paris} edge from parent node[left] {\scriptsize $C$}}
        } edge from parent node[left] {\scriptsize $A$} }
      } edge from parent node[right] {\scriptsize $H$} }
      ;
    \draw[dashed,->] (graduation) to node [auto] {\scriptsize $A$} (John);
    \node (LKG) at (-1.8,0) [fill=black!20, circle] {};
          \draw[bend right] (LKG) to node [auto, left] {\scriptsize $LR$} (After);
          \draw (LKG) to[out=-60, in=190] node [below] {\scriptsize $LA\quad$} (graduation);
          \draw (LKG) to[out=30, in=90] node [above] {\scriptsize $LA$} (moved);
  \end{tikzpicture}
  \caption{UCCA example with linkage.}
  \label{fig:example_linkage}
\end{figure}

The linkage relation is represented by the gray node.
$LA$ is \emph{link argument}, and $LR$ is \emph{link relation}.
The relation represents the fact that the \emph{linker} ``After'' links the two parallel scenes
that are the arguments of the linkage.
Linkage relations are another source of multiple parents for a node, which we do not yet handle
in parsing and evaluation.

\paragraph{Implicit units.}

UCCA graphs may contain implicit units with no correspondent in the text.
\figref{fig:example_implicit} shows the annotation for the sentence
``A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.''.
The sentence was used by \citet{oepen2015semeval} to compare between different semantic
dependency schemes.
It includes a single scene, whose main relation is ``apply'', a secondary relation ``almost impossible'', as well as two complex arguments: ``a similar technique'' and the coordinated argument ``such as cotton, soybeans, and rice.''
In addition, the scene includes an implicit argument, which represents the agent of the
``apply'' relation.

The parsing of these units is deferred to future work, as it is likely to require different methods
than those explored in this paper \cite{roth2015inducing}.

\begin{figure*}
  \centering
  \scalebox{.6}{
  \begin{tikzpicture}[level distance=20mm, ->,
  level 1/.style={sibling distance=8em},
  level 2/.style={sibling distance=4em},
  level 3/.style={sibling distance=4em}]
    \node (ROOT) [fill=black, circle] {}
      child {node [fill=black, circle] {}
      {
        child {node {A} edge from parent node[left] {\scriptsize $E$}}
        child {node {similar} edge from parent node[left] {\scriptsize $E$}}
        child {node {technique} edge from parent node[right] {\scriptsize $C$}}
      } edge from parent node[left] {\scriptsize $A\quad$ \hspace{1mm} } }
      child {node {is} edge from parent node[left] {\scriptsize $F$}}
      child {node [fill=black, circle] {}
      {
        child {node {almost} edge from parent node[left] {\scriptsize $E$}}
        child {node {impossible} edge from parent node[right] {\scriptsize $C$}}
      } edge from parent node[left] {\scriptsize $D$} }
      child {node {\textbf{IMPLICIT}} edge from parent node[left] {\scriptsize $A$}}
      child {node {to} edge from parent node[left] {\scriptsize $F$}}
      child {node {apply} edge from parent node[left] {\scriptsize $P\quad$}}
      child {node [fill=black, circle] {}
      {
        child {node {to} edge from parent node[left] {\scriptsize $R$}}
        child {node {other} edge from parent node[left] {\scriptsize $E$}}
        child {node {crops} edge from parent node[left] {\scriptsize $C$}}
        child {node {,} edge from parent node[left] {\scriptsize $U$}}
        child {node [fill=black, circle] {}
        {
          child {node {such as} edge from parent node[left] {\scriptsize $R$}}
          child {node {cotton} edge from parent node[left] {\scriptsize $C$}}
          child {node {,} edge from parent node[left] {\scriptsize $U$}}
          child {node {soybeans} edge from parent node[left] {\scriptsize $C$}}
          child {node {and} edge from parent node[left] {\scriptsize $N$}}
          child {node {rice} edge from parent node[right] {\scriptsize $\; C$}}
        } edge from parent node[right] {\scriptsize $\; E$ \hspace{1mm} } }
      } edge from parent node[left] {\scriptsize $A\;$ \hspace{1mm} } }
      child {node {.} edge from parent node[right] {\scriptsize $\quad \quad U$}}
      ;
  \end{tikzpicture}
  }
  \caption{UCCA example with an implicit unit.}
  \label{fig:example_implicit}
\end{figure*}

\begin{table}[ht]
\centering
\begin{tabular}{l|ccc|c}
& \multicolumn{3}{c|}{Wiki} & 20K \\
& \small Train & \small Dev & \small Test & Leagues \\
\hline
nodes \\
\# implicit & 899 & 122 & 77 & 241 \\
\# linkage & 2956 & 263 & 359 & 376 \\
\hline
edges \\
\# linkage & 9276 & 803 & 1094 & 957
\end{tabular}
\caption{Statistics of linkage and implicit nodes in the
\textit{Wiki} and \textit{20K Leagues} UCCA corpora.
Cf. \tabref{table:data}.
}
\label{table:data_linkage_implicit}
\end{table}

\section{Hyperparameter Values}
\label{appendix:hyperparameters}

\tabref{table:hyperparameters} lists the hyperparameter values we found
for the different classifiers by tuning on the development set.
Note that learning rate decay is multiplicative and is applied at each epoch.
Mini-batch size is in number of transitions,
but a mini-batch must contain only whole sentences.

\begin{table}
\scalebox{.9}{
\begin{tabular}{l|ccc}
& Sparse & MLP & BiLSTM \\
\hline
\multicolumn{4}{l}{\footnotesize Embedding dimensions} \\
external word & & 100 & 100 \\
word & & 200 & 200 \\
POS tag & & 20 & 20 \\
syntactic dep. & & 10 & 10 \\
edge label & & 20 & 20 \\
punctuation & & 1 & 1 \\
gap & & 3 & 3 \\
action & & 3 & 3 \\
\hline
\multicolumn{4}{l}{\footnotesize Other parameters} \\
training epochs & 19 & 28 & 59 \\
$\textsc{MinUpdate}$ & 5 \\
initial learning rate & 1 & 1 & 1 \\
learning rate decay & 0.1 & 1 & 1 \\
MLP \#layers & & 2 & 2 \\
MLP layer dim. & & 100 & 50 \\
LSTM \#layers & & & 2 \\
LSTM layer dim. & & & 500 \\
word dropout & & 0.2 & 0.2 \\
dropout & & 0.4 & 0.4 \\
weight decay & & $10^{-5}$ & $10^{-5}$ \\
mini-batch size & & 100 & 100
\end{tabular}
}
\caption{Hyperparameters used for the different classifiers.}
\label{table:hyperparameters}
\end{table}

\section{Bilexical Graph Conversion}
\label{appendix:conversion}

Here we describe the algorithms used in the conversion referred to in \secref{sec:exp_setup}.

\paragraph{Notation.}
Let $L$ be the set of possible edge labels.
A UCCA graph over a sequence of tokens $w_1, \ldots, w_n$ is a directed acyclic graph
$G=(V,E, \ell)$, where $\ell:E\to L$ maps edges to labels.
For each token $w_i$ there exists a leaf (\emph{terminal}) $t_i \in V$.
A bilexical (dependency) graph over the same text consists of a set $A$ of
labeled dependency arcs $(t^\prime,l,t)$
between the terminals of $G$, where $t^\prime$ is the head, $t$ is the dependent and $l$ is
the edge label.

\paragraph{Conversion to bilexical graphs.}
Let $G=(V,E,\ell)$ be a UCCA graph with labels $\ell:E\rightarrow L$.
The conversion to a bilexical graph requires calculating the set $A$.
All non-terminals in $G$ are removed.

We define a linear order over possible edge labels $L$ (see \figref{fig:priority}).
For each node $u \in V$, denote by $h(u)$ its child with the highest-priority edge label.
Let $h^*(u)$ be the terminal reached by recursively applying $h(\cdot)$ over $u$.
For each terminal $t$, we define
\[
N(t) = \{(u,v)\in E \;|\; t=h^*(v) \wedge t \neq h^*(u) \}
\]
For each edge $(u,v)\in N(t)$, we add $h^*(u)$ as a head of $t$ in $A$,
with the label $\ell(u,v)$.
This procedure is given in Algorithm~\ref{alg:to_bilexical}.

\begin{algorithm}[ht]
 \KwData{UCCA graph ${G}=(V,E,\ell)$}
 \KwResult{set $A$ of labeled bilexical arcs}
 $A \leftarrow \emptyset$\;
 \ForEach{$t \in \mathrm{Terminals}(V)$} {
  \ForEach{$(u,v)\in N(t)$} {
   $A \leftarrow A \cup \{(h^*(u), \ell(u, v), t)\}$\;
  }
 }
 \caption{Conversion to bilexical graphs.}
 \label{alg:to_bilexical}
\end{algorithm}

Note that this conversion procedure
is simpler than the head percolation procedure used for converting syntactic constituency
trees to dependency trees \cite{Coll:97},
since $h(u)$ (similar to $u$'s head-containing child)
depends only on $\ell(u,h(u))$ and not on the sub-tree spanned by $u$,
because edge labels in UCCA directly express the role of the child in the parent unit, and
are thus sufficient for determining which of $u$'s children contains the head node.

\paragraph{Conversion from bilexical graphs.}
The inverse conversion introduces non-terminal nodes back into the graph.
As the distinction between low- and high-attaching nodes is lost in the
conversion, we assume that attachments are always
low-attaching.
Let $A$ be a the labeled arc set of a bilexical graph.
Iterating over the terminals in topological order according to $A$,
we add its members as terminals to graph
and create a pre-terminal parent $u_t$ for each terminal $t$,
with an edge labeled as \textit{Terminal} between them.
The parents of the pre-terminals are determined by the terminal's parent in the bilexical
graph: if $t^\prime$ is a head of $t$ in $A$, then $u_{t^\prime}$ will be a parent of $u_t$.
We add an intermediate node in between if $t$ has any dependents in $A$,
to allow adding their pre-terminals as children later.
Edge labels for the intermediate edges are determined by a rule-based function, denoted by
$\mathrm{Label}(t)$.
This procedure is given in Algorithm~\ref{alg:from_bilexical}.

\begin{algorithm}[ht]
 \KwData{list $T$ of terminals, set $A$ of labeled bilexical arcs}
 \KwResult{UCCA graph $G=(V,E,\ell)$}
 \SetKwFunction{Label}{Label}{}{}
 $V \leftarrow \emptyset$\;
 $E \leftarrow \emptyset$\;
 \ForEach{$t \in \mathrm{TopologicalSort}(T, A)$} {
  $u_t \leftarrow \mathrm{Node()}$\;
  $V \leftarrow V \cup \{u_t, t\}$\;
  $E \leftarrow E \cup \{(u_t, t)\}$\;
  $\ell(u_t,t)\leftarrow\mathit{Terminal}$\;
  \ForEach{$t^\prime\in T,l\in L$} {
   \If{$(t^\prime,l,t)\in A$} {
    \eIf{$\exists t^{\prime\prime}\in T,l^\prime\in L : (t,l^\prime,t^{\prime\prime}) \in A$} {
     $u \leftarrow \mathrm{Node()}$\;
     $V \leftarrow V \cup \{u\}$\;
     $E \leftarrow E \cup \{(u, u_t)\}$\;
     $\ell(u, u_t) \leftarrow \Label(t)$\;
    } {
     $u \leftarrow u_t$\;
    }
    $E \leftarrow E \cup \{(u_{t^\prime}, u)\}$\;
    $\ell(u_{t^\prime}, u) \leftarrow l$\;
    }
  }
 }
 
  \SetKwProg{func}{Function}{}{}
  
  \func{\Label}{
  \KwData{node $t \in T$}
  \KwResult{label $l\in L$}

   \uIf{$\mathrm{IsPunctuation}(t)$}{
    \Return \textit{Punctuation}\;
   }
   \uElseIf{$\exists t^\prime \in T : (t,\textit{ParallelScene},t^\prime)\in A$}{
    \Return \textit{ParallelScene}\;
   }
   \uElseIf{$\exists t^\prime \in T : (t,\textit{Participant},t^\prime)\in A$}{
    \Return \textit{Process}\;
   }
   \uElse{
    \Return \textit{Center}\;
   }
  }
 \caption{Conversion from bilexical graphs.}
 \label{alg:from_bilexical}
\end{algorithm}

\begin{figure}[ht]
\begin{enumerate}
\itemsep0em
\item $C$ (Center)
\item $N$ (Connector)
\item $H$ (ParallelScene)
\item $P$ (Process)
\item $S$ (State)
\item $A$ (Participant)
\item $D$ (Adverbial)
\item $T$ (Time)
\item $E$ (Elaborator)
\item $R$ (Relator)
\item $F$ (Function)
\item $L$ (Linker)
\item $LR$ (LinkRelation)
\item $LA$ (LinkArgument)
\item $G$ (Ground)
\item $\mathit{Terminal}$ (Terminal)
\item $U$ (Punctuation)
\end{enumerate}
\caption{Priority order of edge labels used by $h(u)$.}
\label{fig:priority}
\end{figure}

\end{document}
