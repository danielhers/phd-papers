%
% File acl2018_supp.tex
%

\documentclass[11pt,a4paper]{article}
\usepackage{acl-onecolumn}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{pgfplotstable}
%\usepackage{algorithm2e}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{float}
\usepackage{lipsum,adjustbox}
\usepackage{tikz}
\usepackage{tikz-dependency}
\usepackage{enumitem}
\usepackage{xr}
\externaldocument{tupa_multitask}
\usetikzlibrary{shapes,fit,calc,er,positioning,intersections,decorations.shapes,mindmap,trees}
\tikzset{decorate sep/.style 2 args={decorate,decoration={shape backgrounds,shape=circle,
      shape size=#1,shape sep=#2}}}
\newcommand{\oa}[1]{\footnote{\color{red} #1}}
\newcommand{\daniel}[1]{\footnote{\color{blue} #1}}
\newcommand{\com}[1]{}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
%\SetKwRepeat{Do}{do}{while}

\hyphenation{SemEval}
\hyphenation{PARSEVAL}

\title{Multitask Parsing across Semantic Representations \\ Supplementary Notes}

\begin{document}
\maketitle

\paragraph{Features.}

Table~\ref{tab:features} lists all feature used for the classifier (see \S\ref{sec:classifier}).
Numeric features are taken as they are, whereas categorical features are mapped to real-valued embedding
vectors.
For \texttt{w} features,
we concatenate randomly-initialized and pre-trained word embeddings.
For each node, we select a \textit{head terminal} by traversing the graph according to
a priority order on edge labels, taken from \citet{hershcovich2017a}.

$s_i$ refers to stack node $i$ from the top, and
$b_i$ to buffer node $i$.
$xl$ and $xr$ refer to a $x$'s leftmost and rightmost children, and
$xL$ and $xR$ to its leftmost and rightmost parents.

\texttt{w} refers to the node's head terminal text,
\texttt{t} to its POS tag, and
\texttt{d} to its dependency relation.
\texttt{h} refers to the node's height,
\texttt{e} to the tag of its first incoming edge,
\texttt{n} and \texttt{c} to the node label and category (used only for AMR),
\texttt{p} to any separator punctuation between $s_0$ and $s_1$,
\texttt{q} to the count of any separator punctuation between $s_0$ and $s_1$,
\texttt{x} to the numeric value of gap type \cite{maier-lichte:2016:DiscoNLP},
\texttt{y} to the sum of gap lengths,
\texttt{P}, \texttt{C}, \texttt{I}, \texttt{E}, and \texttt{M} to the number of
parents, children, implicit children, remote children, and remote parents,
\texttt{N} to the numeric value of the head terminal's named entity IOB indicator,
\texttt{T} to its named entity type,
\texttt{\#} to its word shape (capturing orthographic features, e.g. "Xxxx" or "dd"),
\texttt{\^{}} to its one-character prefix, and
\texttt{\$} to its three-character suffix.

$x \to y$ refers to the existing edge from $x$ to $y$.
\texttt{x} is an indicator feature, taking the value of 1 if the edge exists or 0 otherwise,
\texttt{e} refers to the edge label, and
\texttt{d} to the dependency distance between the head terminals of the nodes.

$a_i$ to the transition taken $i+1$ steps ago.
\texttt{A} refers to the action type label (e.g. \textsc{shift}/\textsc{right-edge}/\textsc{node}), and
\texttt{e} to the edge label created by the action (e.g. $C$/$E$/$P$).

\texttt{node ratio} is the ratio between non-terminals and terminals, taken from \citet{hershcovich2017a}.

\begin{table}[h]
\centering
\begin{tabular}{l|l}
\bf Nodes & \bf Features \\ 
\hline
%       w   t   d   e   n   c   p   T   #   ^   $   x   h   q   y   P   C   I   E   M   N
$s_0$ & \texttt{wtdencpT\#\^{}\$xhqyPCIEMN} \\
$s_1$ & \texttt{wtdencT\#\^{}\$xhyN} \\
$s_2$ & \texttt{wtdencT\#\^{}\$xhy} \\
$s_3$ & \texttt{wtdencT\#\^{}\$xhyN} \\
$b_0$ & \texttt{wtdncT\#\^{}\$hPCIEMN} \\
$b_1, b_2, b_3$ & \texttt{wtdncT\#\^{}\$} \\
$s_0l, s_0r, s_1l, s_1r, s_0ll, s_0lr, s_0rl, s_0rr, s_1ll, s_1lr, s_1rl, s_1rr$ &
    \texttt{wenc\#\^{}\$} \\
$s_0L, s_0R, s_1L, s_1R, b_0L, b_0R$ & \texttt{wen\#\^{}\$} \\
\hline
\bf Edges & \\
$s_0 \to s_1, s_0 \to b_0$ & \texttt{xd} \\
$s_1 \to s_0, b_0 \to s_0$ & \texttt{x} \\
$s_0 \to b_0, b_0 \to s_0$ & \texttt{e} \\
\hline
\bf Past actions \\
$a_0, a_1$ & \texttt{eA} \\
\hline
\bf Misc. & \texttt{node ratio}
\end{tabular}
\caption{Transition classifier features.\label{tab:features}}
\end{table}

Hyperparameter settings are listed in Table~\ref{tab:hyperparams}.
The middle column shows the hyperparameters used for the single-task architecture,
described in \S\ref{sec:classifier}, and
the right column for the multitask architecture,
described in \S\ref{sec:multitask}.
\textbf{Main} refers to parameters specific to the main task---UCCA parsing
(task-specific MLP and BiLSTM, and edge label embedding),
\textbf{Aux} to parameters specific to each auxiliary task
(task-specific MLP and BiLSTM, but no edge label embedding since the tasks are unlabeled),
and \textbf{Shared} to parameters shared among all tasks
(shared BiLSTM and embeddings).

\begin{table}[h]
\centering
\begin{tabular}{l|c|ccccc}
&& \multicolumn{3}{c}{\bf Multitask} \\ 
\bf Hyperparameter &  \bf Single & \bf Main & \bf Aux & \bf Shared \\
\hline
Pre-trained word embedding dimensions & 300 &&& 300 \\
Randomly initialized word embedding dimensions & 200 &&& 200 \\
POS tag embedding dimensions & 20 &&& 20 \\
Syntactic dependency embedding dimensions & 10 &&& 10 \\
Named entity type embedding dimensions & 3 &&& 3 \\
Punctuation embedding dimensions & 1 &&& 1 \\
Action embedding dimensions & 3 &&& 3 \\
Edge label embedding dimensions & 20 & 20 \\
\hline
MLP layers & 2 & 2 & 1 \\
MLP dimensions & 50 & 50 & 50 \\
BiLSTM layers & 2 & 2 & 1 & 2 \\
BiLSTM dimensions & 500 & 250 & 50 & 250
\end{tabular}
\caption{Hyperparameter settings.\label{tab:hyperparams}}
\end{table}

\paragraph{Conversion to and from Unified DAG Format.}

Although all experiments reported in the paper with the auxiliary tasks
(AMR, DM and UD) are using unlabeled parsing for these schemes,
our conversion code,
which is attached as supplementary material and is also publicly available at $<$anonymized$>$,
supports full conversion to and from these formats
(see \texttt{scheme/conversion} there).

Conversion from AMR to the unifid DAG format and back
results in 95\% Smatch $F_1$ \cite{cai2013smatch} when averaged over the
LDC2017T10 test set.
On SDP, the conversion is lossless and results in identical graphs
when converted to UCCA and back.
For UD, and conversion results in 98.5\% LAS $F_1$ on the UD English test set,
due to multi-word tokens, not supported in the unified DAG format.

\bibliography{references}
\bibliographystyle{acl_natbib}

\end{document}
