%
% File acl2018_supp.tex
%

\documentclass[11pt,a4paper]{article}
\usepackage{acl-onecolumn}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{pgfplotstable}
%\usepackage{algorithm2e}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{float}
\usepackage{lipsum,adjustbox}
\usepackage{tikz}
\usepackage{tikz-dependency}
\usepackage{enumitem}
\usepackage{xr}
\externaldocument{tupa_multitask}
\usetikzlibrary{shapes,fit,calc,er,positioning,intersections,decorations.shapes,mindmap,trees}
\tikzset{decorate sep/.style 2 args={decorate,decoration={shape backgrounds,shape=circle,
      shape size=#1,shape sep=#2}}}
\newcommand{\oa}[1]{\footnote{\color{red} #1}}
\newcommand{\daniel}[1]{\footnote{\color{blue} #1}}
\newcommand{\com}[1]{}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
%\SetKwRepeat{Do}{do}{while}

\hyphenation{SemEval}
\hyphenation{PARSEVAL}

\title{Multitask Parsing across Semantic Representation Schemes \\ Supplementary Notes}

\begin{document}
\maketitle


\section*{Supplementary Notes}

\paragraph{Feature Templates.}

Table~\ref{tab:features} lists all feature used for the classifier (see \S\ref{sec:classifier}).
Numeric features are taken as they are, whereas categorical features are mapped to real-valued embedding
vectors.
For \texttt{w} features,
we use both randomly-initialized and pre-trained word embeddings, concatenating them.
For each node, we select a \textit{head terminal} by traversing down the graph according to
a heuristically determined priority order, taken from \citet{hershcovich2017a}.

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{l|l}
\bf Nodes & \bf Features \\ 
\hline
%       w   t   d   e   n   c   p   T   #   ^   $   x   h   q   y   P   C   I   E   M   N
$s_0$ & \texttt{wtdencpT\#\^{}\$xhqyPCIEMN} \\
$s_1$ & \texttt{wtdencT\#\^{}\$xhyN} \\
$s_2$ & \texttt{wtdencT\#\^{}\$xhy} \\
$s_3$ & \texttt{wtdencT\#\^{}\$xhyN} \\
$b_0$ & \texttt{wtdncT\#\^{}\$hPCIEMN} \\
$b_1, b_2, b_3$ & \texttt{wtdncT\#\^{}\$} \\
\multirow{3}{3cm}{$s_0l, s_0r, s_1l, s_1r$, $s_0ll, s_0lr, s_0rl, s_0rr$, $s_1ll, s_1lr, s_1rl, s_1rr$} &
    \texttt{wenc\#\^{}\$} \\ \\ \\
\multirow{2}{3cm}{$s_0L, s_0R, s_1L, s_1R$, $b_0L, b_0R$} & \texttt{wen\#\^{}\$} \\ \\
\hline
\bf Edges & \\
$s_0 \to s_1, s_0 \to b_0$ & \texttt{xd} \\
$s_1 \to s_0, b_0 \to s_0$ & \texttt{x} \\
$s_0 \to b_0, b_0 \to s_0$ & \texttt{e} \\
\hline
\bf Past actions \\
$a_0, a_1$ & \texttt{eA} \\
\hline
\bf Misc. & \texttt{node ratio}
\end{tabular}
\caption{Transition classifier features.\label{tab:features}
\\ \footnotesize
$s_i$ refers to stack node $i$ from the top, and
$b_i$ to buffer node $i$.
$xl$ and $xr$ refer to a $x$'s leftmost and rightmost children, and
$xL$ and $xR$ to its leftmost and rightmost parents.
\\
\texttt{w} refers to the node's head terminal text,
\texttt{t} to its POS tag, and
\texttt{d} to its dependency relation.
\texttt{h} refers to the node's height,
\texttt{e} to the tag of its first incoming edge,
\texttt{n} and \texttt{c} to the node label and category (used only for AMR),
\texttt{p} to any separator punctuation between $s_0$ and $s_1$,
\texttt{q} to the count of any separator punctuation between $s_0$ and $s_1$,
\texttt{x} to the numeric value of gap type \cite{maier-lichte:2016:DiscoNLP},
\texttt{y} to the sum of gap lengths,
\texttt{P}, \texttt{C}, \texttt{I}, \texttt{E}, and \texttt{M} to the number of
parents, children, implicit children, remote children, and remote parents,
\texttt{N} to the numeric value of the head terminal's named entity IOB indicator,
\texttt{T} to its named entity type,
\texttt{\#} to its word shape (capturing orthographic features, e.g. "Xxxx" or "dd"),
\texttt{\^{}} to its one-character prefix, and
\texttt{\$} to its three-character suffix.
\\
$x \to y$ refers to the existing edge from $x$ to $y$.
\texttt{x} is an indicator feature, taking the value of 1 if the edge exists or 0 otherwise,
\texttt{e} refers to the edge label, and
\texttt{d} to the dependency distance between the head terminals of the nodes.
\\
$a_i$ to the transition taken $i+1$ steps ago.
\texttt{A} refers to the action type label (e.g. \textsc{shift}/\textsc{right-edge}/\textsc{node}), and
\texttt{e} to the edge label created by the action (e.g. $C$/$E$/$P$).
\\
\texttt{node ratio} is the ratio between non-terminals and terminals, taken from \citet{hershcovich2017a}.
}
\end{table}

Hyperparameter settings are listed in Table~\ref{tab:hyperparams}.
The middle column shows the hyperparameters used for the single-task architecture,
described in \S\ref{sec:classifier}, and
the right column for the multitask architecture,
described in \S\ref{sec:multitask}.
\textbf{Main} refers to parameters specific to the main task---UCCA parsing
(task-specific MLP and BiLSTM, and edge label embedding),
\textbf{Aux} to parameters specific to each auxiliary task
(task-specific MLP and BiLSTM, but no edge label embedding since the tasks are unlabeled),
and \textbf{Shared} to parameters shared among all tasks
(shared BiLSTM and embeddings).

External word embeddings are pre-trained word embeddings (see \S\ref{sec:training}),
and word embeddings are randomly initialized.

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{l|c|ccccc}
\bf Hyperparameter &  \bf Single & \multicolumn{3}{c}{\bf Multitask} \\ 
&& \bf Main & \bf Aux & \bf Shared \\
\hline
external word dim. & 300 &&& 300 \\
word dim. & 200 &&& 200 \\
POS tag dim. & 20 &&& 20 \\
syntactic dep. dim. & 10 &&& 10 \\
named entity dim. & 3 &&& 3 \\
punctuation dim. & 1 &&& 1 \\
action dim. & 3 &&& 3 \\
edge label dim. & 20 & 20 \\
MLP layers & 2 & 2 & 1 \\
MLP dimensions & 50 & 50 & 50 \\
BiLSTM layers & 2 & 2 & 1 & 2 \\
BiLSTM dimensions & 500 & 250 & 50 & 250
\end{tabular}
\caption{Hyperparameter settings.\label{tab:hyperparams}
Middle column: single-task.
Right column: multitask settings for main task (UCCA), auxiliary tasks, and shared parameters.}
\end{table}

\paragraph{Conversion to and from Unified DAG Format.}

Although all experiments reported in the main paper on the auxiliary tasks
(AMR, DM and UD) are using unlabeled parsing for these schemes,
our conversion code,
which is attached as supplementary material and is also publicly available at $<$anonymized$>$,
supports full conversion to and from these formats
(see \texttt{scheme/conversion} in the supplementary software).

Conversion from AMR to the unifid DAG format and back
results in 95\% Smatch $F_1$ \cite{cai2013smatch} when averaged over the
LDC2017T10 test set.
On SDP, the conversion is lossless and results in identical graphs
when converted to UCCA and back.
For UD, and conversion results in 98.5\% LAS $F_1$ on the UD{\_}English test set,
due to multi-word tokens, not supported in the unified DAG format.

\bibliography{references}
\bibliographystyle{acl_natbib}

\end{document}
