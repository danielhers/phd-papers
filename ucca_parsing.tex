%
% File ucca_parsing.tex
%

\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{pgfplotstable}
\usepackage[]{algorithm2e}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{color}
\usepackage{lipsum,adjustbox}
\usepackage{tikz}
\usepackage{tikz-dependency}
\usetikzlibrary{shapes,fit,calc,er,positioning,intersections,decorations.shapes,mindmap,trees}
\tikzset{decorate sep/.style 2 args={decorate,decoration={shape backgrounds,shape=circle,shape size=#1,shape sep=#2}}}

\newcommand{\oa}[1]{\footnote{\color{red} #1}}
\newcommand{\daniel}[1]{\footnote{\color{blue} #1}}
\newcommand{\com}[1]{}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\SetKwRepeat{Do}{do}{while}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Broad-Coverage Semantic Constituency Parsing: \\
A Transition-Based Approach}
  %General Transition-Based Broad-Coverage Semantic Parsing

\author{Daniel Hershcovich \and Omri Abend \and Ari Rappoport \\
  Institute of Computer Science \\
  Hebrew University of Jerusalem \\
  {\tt \{danielh,oabend,arir\}@cs.huji.ac.il}
}

\date{}


\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%     Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

  Representing many common semantic phenomena requires more expressive formalisms
  than those commonly used for syntactic parsing, motivating research into
  more formally expressive parsers.
  Recent work has focused on broad-coverage semantic parsing into dependency
  (i.e., word-to-word) or abstract (i.e., not grounded in the words and phrases of
  the sentence) representations.
  Instead, we advocate a constituency-based formulation of the
  task and define the broad-coverage constituency semantic parsing task
  (henceforth, {\it BCSP}) using the UCCA semantic scheme and annotated corpora as a testbed.
  We note that there are no existing methods for constituency DAG parsing,
  and propose two transition-based approaches to tackle it:
  (1) creating conversion tools for converting UCCA structures into related
  formalisms and back, and using existing state-of-the-art parsers for
  these settings, and (2) constructing a novel parser for the task.
  
  %We experiment with two types of semantic representations, obtaining
  %encouraging results.
  %Universal Conceptual Cognitive Annotation (UCCA) is a semantic grammatical scheme
  % that assigns 
  %a complete structure to natural language text. We present the first automatic
  % UCCA parser, 
  % a transition-based parser using a novel transition system. We compare the
  % results to baselines 
  % obtained by converting UCCA to CoNNL-X, and training syntactic parsers on the
  % converted dependency trees.
  
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% (1) there has been much interest lately in broad-coverage semantic parsing.
%  (2) semantic parsing requires the development of new parsing technology
% (beyond syntactic parsers), as they are formally different. importantly, they
%  are not trees, but DAGs (e.g., argument sharing in control structures;
% see Oepen et al., 2015, AMR).

The formal constraints imposed by available statistical parsers are too restrictive
broad-coverage semantic parsing \cite{oepen2015semeval}.
For instance, arguments that are shared
between predicates are generally represented as multi-parented nodes,
yielding DAG, rather than tree structures. This has motivated much recent
inquiry into semantic parsing with more expressive formalisms.

Earlier work on statistical broad-coverage semantic parsing has mostly
concentrated on shallow semantic analysis, focusing on verb argument structure phenomena.
Recently work has focused on parsing into a more elaborate representation, accounting
for other phenomena, such as inter-clause relations and non-verbal predicates.
Much recent interest has been placed on parsing sentences into abstract meaning representations,
not grounded in the words and phrases, notably AMR \cite{banarescu2013abstract}.
While sharing much of these works' motivation, AMR poses difficulties as it conflates
a wide variety of linguistic phenomena into a single unlayered representation.
Moreover, not grounding the text within the semantic representation,
requires that the alignment between words and logical symbols be automatically
(and imprecisely) detected, and may consequently complicate the parsing task.

An alternative line of work tackles parsing into grounded semantic representations.
Most work of this strand represented semantics using dependency structures.
Various methods have been systematically explored
in two recent SemEval shared tasks \cite{oepen2014semeval,oepen2015semeval}.
Bi-lexical dependencies are indeed an appealing means for representing semantics, partly due to
their structural and conceptual simplicity and the strong and efficient
methods they allow for (see \secref{sec:related}).
However, the assumption that each semantic unit has a unique head is
problematic for representing several frequent constructions that have no clear
head, such as coordination, prepositional phrases and multi-word expressions,
leading to unsystematic treatment of these cases, which yields both conceptual
and practical problems \cite{schwartz2011neutralizing,Ivanova2012who,tsarfaty2012cross}.

In this work we define and tackle the constituency-based semantic parsing task.
Constituency semantic representation is motivated both as it does not
impose the limiting unique head assumption, and its grounding in the text,
which allows for the use of strong and efficient parsing methods.
Indeed, transition-based methods, which have recently produced some of the best
results in syntactic dependency parsing \cite{dyer2015transition,ballesteros2015improved},
can be applied
for constituency parsing as well \cite{sagae2005classifier,zhu2013fast,maier2015discontinuous}. 
We follow the transition-based approach, motivated by its conceptual simplicity,
efficiency and high performance.

We use corpora annotated with the UCCA scheme \cite{abend2013universal} as a test
case\footnote{In fact, while the task definition is not specific to UCCA,
  to the best of our knowledge, these are the only
  semantically annotated corpora that use constituency structures.}
and pursue two complementary parsing strategies.
First, in order to assess the ability of existing technology to tackle the task,
we build converters between the UCCA annotation and two related formalisms:
\textsc{CoNLL}-style dependencies and discontiguous constituency trees.
We are not aware of any statistical constituency DAG parsers, and specifically not
ones that support discontiguous constituents, as required for a broad-coverage semantic
parser (see, e.g., \cite{pitler2015linear,maier2015discontinuous}, for parsers motivated by discontiguous units). Therefore, the conversion is necessarily lossy, but, as we show, is still effective
in practice (\secref{sec:conversion_approach}).
Second, we present a novel transition-based
constituency DAG parser that supports discontiguous constituents (\secref{sec:direct_approach}). 
We apply the parser both to an in-domain and out-of-domain
scenario, with encouraging results.

%A general parser for this setting must support the prediction of multiple parents,
%as well as of discontiguous constituents, both pervasive phenomena in semantic
%structures even in English (see \secref{sec:data}.
%Multiple parents show up frequently in the context of argument
%sharing, where a certain argument participates in several relations but is only mentioned once.
%Common examples are coordination structures
%(e.g., in ``John went home and took a shower'', ``John'' is an argument of both ``went'' and ``took'')
%or nominalizations (e.g., ``After graduation, John moved to London'', where ``John'' is an argument
%of both ``graduation'' and ``moved'').
%Discontiguous units are common even in syntactic structures in some languages (e.g., German, Bulgarian,
%Korean and many others \cite{kallmeyer2013data}), and can even be found in English, e.g., in reported
%speech: ``'The dog', John said, 'is back again''', and are even more pervasive in English semantic parses
%(see \secref{sec:data}).

The contribution of this work is thus three-fold.
First, we motivate and define the broad-coverage semantic constituency parsing task.
Second, we create converters and assess the ability of existing parsing technology
for formally related settings to address the task, focusing on transition-based methods.
Third, we present the first constituency DAG parser, which is also the
first parser for UCCA. 
All converters, parsers and data sets will be made publicly available upon publication.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}\label{sec:related}

\begin{figure*}
\begin{adjustbox}{width=\textwidth}
  \begin{tikzpicture}[level distance=20mm, ->,
  level 1/.style={sibling distance=8em},
  level 2/.style={sibling distance=4em},
  level 3/.style={sibling distance=4em}]
    \label{fig:similar_technique}
    \node (ROOT) [fill=black, circle] {}
      child {node [fill=black, circle] {}
      {
        child {node {A} edge from parent node[left] {\scriptsize $E$}}
        child {node {similar} edge from parent node[left] {\scriptsize $E$}}
        child {node {technique} edge from parent node[right] {\scriptsize $C$}}
      } edge from parent node[left] {\scriptsize $A\quad$ \hspace{1mm} } }
      child {node {is} edge from parent node[left] {\scriptsize $F$}}
      child {node [fill=black, circle] {}
      {
        child {node {almost} edge from parent node[left] {\scriptsize $E$}}
        child {node {impossible} edge from parent node[right] {\scriptsize $C$}}
      } edge from parent node[left] {\scriptsize $D$} }
      child {node {\textbf{IMPLICIT}} edge from parent node[left] {\scriptsize $A$}}
      child {node {to} edge from parent node[left] {\scriptsize $F$}}
      child {node {apply} edge from parent node[left] {\scriptsize $P\quad$}}
      child {node [fill=black, circle] {}
      {
        child {node {to} edge from parent node[left] {\scriptsize $R$}}
        child {node {other} edge from parent node[left] {\scriptsize $E$}}
        child {node {crops} edge from parent node[left] {\scriptsize $C$}}
        child {node {,} edge from parent node[left] {\scriptsize $U$}}
        child {node [fill=black, circle] {}
        {
          child {node {such as} edge from parent node[left] {\scriptsize $R$}}
          child {node {cotton} edge from parent node[left] {\scriptsize $C$}}
          child {node {,} edge from parent node[left] {\scriptsize $U$}}
          child {node {soybeans} edge from parent node[left] {\scriptsize $C$}}
          child {node {and} edge from parent node[left] {\scriptsize $N$}}
          child {node {rice} edge from parent node[right] {\scriptsize $\; C$}}
        } edge from parent node[right] {\scriptsize $\; E$ \hspace{1mm} } }
      } edge from parent node[left] {\scriptsize $A\;$ \hspace{1mm} } }
      child {node {.} edge from parent node[right] {\scriptsize $\quad \quad U$}}
      ;
  \end{tikzpicture}
\end{adjustbox}
\caption{\label{fig:ucca_example}
  UCCA annotation of the sentence ``A similar technique is almost impossible to
  apply to other crops, such as cotton, soybeans and rice.''.
  The sentence was used by Oepen et al. (2015) to compare between the difference schemes. The sentence includes a single Scene, whose main relation is ``apply'', a secondary relation ``almost impossible'', as well as two complex arguments: ``a similar technique'' and the coordinated argument ``such as cotton, soybeans, and rice''.
}
\end{figure*}

\paragraph{Broad-coverage Deep Semantic Representation.}
The Abstract Meaning Representation (AMR) scheme \cite{banarescu2013abstract},
provides deep semantic annotation in the form of elaborate graphs,
which account for a wider range of predicates, linkage and coreference phenomena.
Formally, it represents sentences as directed graphs, without grounding their
semantic sub-parts in the sentence's words and constituents.

Another line of work that addresses parsing into elaborate semantic structures
is that of Broad-coverage Semantic Dependency Parsing (SDP) \cite{oepen2014semeval,oepen2015semeval}.
Like AMR, SDP also addresses a wide range of argument structures (including verbal, nominal and
adjectival ones) and the inter-relations between them, but uses word-to-word dependency structures.
Three major approaches have been explored:
the Prague Dependency Treebank tectogrammatical layer \cite{bohmova2003prague},
HSPG-derived parsers using the Enju parser\footnote{See \url{http://kmcs.nii.ac.jp/enju/}.},
and dependencies derived from the Lingo ERG Minimal Recursion Semantics represenations \cite{Flic:02}.

\paragraph{Universal Conceptual Cognitive Annotation.}
Taking a constituency-based approach to semantic parsing,
we use the annotated UCCA corpora, a broad-coverage semantic annotation scheme,
which focuses on predicate-argument structures (verbal, nominal, adjectival and others)
and the inter-relations between them. UCCA \cite{abend2013universal} builds on an established
framework for typological description, ``Basic Linguistic Theory''
\cite{Dixon:10b,Dixon:10a,Dixon:12}, as well as on the Cognitive Linguistics literature. The scheme
has been shown to support rapid annotation and to be semantically stable in translation: UCCA
annotations of translated text usually contain the same set of relationships
\cite{sulem2015conceptual}.

Formally, a UCCA structure is a constituency DAG, whose leaves (or a subset of them) correspond to the words of the text. The nodes of the graph, called ``units'', are either terminals or several elements (not necessarily contiguous) jointly viewed as a single entity according to some semantic or cognitive consideration. The edges bear a category, indicating the role of the sub-unit in the relation that the parent represents. 
The most basic notion is the ``Scene'', which describes a movement, an action or a state.
Each Scene contains one main relation, or anchor, as well as one or more participants. 
For example, the sentence ``After graduation, John moved to Paris'' contains two Scenes, whose main relations are ``graduation'' and ``moved''. The participant ``John'' is a part of both Scenes, while ``Paris'' only of the latter. Further categories account for relations between Scenes and the internal structures of complex arguments (e.g., coordination) and relations (e.g., complex adverbials, such as ``very clearly'').
\figref{fig:ucca_example} presents another example of a UCCA-annotated sentence. 

\paragraph{Related Semantic and Syntactic Parsing Work.}
Semantic parsing methods can be largely partitioned into grammar-based and grammarless methods.
Within the grammar-based literature, most work relied on Combinatory Categorial Grammar (CCG)
\cite{Steedman:00}, which allows to compute semantic structure compositionally from the
syntactic derivations. Notable examples include the Boxer parser \cite{bos2005towards}
and the AMR parser 
by \newcite{artzi2015broad}. Other examples include parsing with Hyperedge Replacement Grammars
\cite{jones2012semantics,chiang2013parsing,peng2015synchronous} and
graph grammars \cite{koller2015semantic}.
A different line of work takes a discriminative, grammarless approach,
pursuing either graph-based methods that predict the highest ranking graph
(tree or DAG) that satisfies a given set of constraints (e.g., \newcite{flanigan2014discriminative}
for AMR parsing)
or a transition-based method that builds the parse incrementally following a series of local
decisions \cite[and much subsequent work]{Nivre03anefficient}.

Despite being based on local decisions, transition-based methods have yielded excellent
results in a variety of parsing tasks. Within syntactic dependencies parsing, transition-based methods
have been successfully applied to corproa in many languages and domains, yielding some
of the best reported results \cite{dyer2015transition,ballesteros2015improved}. 
The approach has also yielded results comparable with the state-of-the-art when applied
to constituency parsing \cite{sagae2005classifier,zhu2013fast}, and has been recently extended to
discontiguous constituency parsing \cite{maier2015discontinuous},
yielding improvements over in the parsing of discontiguous constituents in German.
Transition-based parsers have also been developed for dependency DAG structures
\cite{sagae2008shift,tokgoz2015transition}, AMR parsing \cite{wang2015transition}
and CCG parsing \cite{ambati2015incremental}.
Nevertheless, there are no constituency DAG parsers, a gap this work addresses.

%Despite the greediness\daniel{is beam search greedy too?} and simplicity of transition-based methods,
%they have yielded excellent
%results in a variety of parsing tasks. Within syntactic dependencies parsing, transition-based methods
%have been successfully applied to corproa in many languages and domains, yielding some
%of the best reported results on this core task \cite{dyer2015transition,ballesteros2015improved}. 
%The approach has also yielded results comparable with the state-of-the-art when applied
%to constituency parsing \cite{sagae2005classifier,zhu2013fast}, and has been recently extended to
%discontiguous constituency parsing \cite{maier2015discontinuous},
%yielding improvements over in the parsing of discontiguous constituents in German.
%Transition-based parsers have also been developed for dependency DAG structures
%\cite{sagae2008shift,tokgoz2015transition}, including a transition-based parser for transforming
%syntactic dependencies into AMR parses \cite{wang2015transition}.
%Nevertheless, to our knowledge no statistical parser has
%been constructed for constituency DAG parsing, a gap which
%we address in this work, presenting a transition-based constituency DAG parser that supports
%the prediction of discontiguous units.


% ADL comment: I removed these paragraphs because I think the text should zero in on precisely what our research methods, without dwelling on background/ previous work, esp. this early in the text. If this appears at all it should be briefly summarized at the end; say how it informs UCCA, but don't dwell on it.

%Ascribing structure to text or speech is an essential component in any linguistic analysis and computational modeling of language. Large-scale structurally annotated corpora began to emerge in the early 90s, and have made an enormous impact on computational linguistics. 
%Most structurally annotated corpora hitherto focused on formulating a set of distinctions applicable to a certain language or genre. A partial reason for the language specificity of the schemes is that most existing treebanks tend to reflect first and foremost syntactic structures, which vary dramatically between languages. 

%This difficulty has resulted in recent years in two major types of work. 
%One is cross-linguistically applicable {\bf syntactic} representation, much of which is currently addressed under the Universal Dependencies project\footnote{\url{http://universaldependencies.github.io/docs/}}. 
%The other is cross-linguistically applicable {\bf semantic} schemes, expressing a shared level of semantic representation, which are therefore more in line with the purposes of this proposal. The most widely used of these is FrameNet \cite{Fillmore:71}, which focuses on lexical semantics with emphasis on cross-linguistic validity \cite{boas2009multilingual}.
%(though see \cite{goddard2011semantic} for critique). 
%While proving a valuable resource, it has limited coverage, even for English, the most studied language \cite{palmer2010evaluating}, and focuses almost exclusively on argument structure phenomena.

%The Abstract Meaning Representation (AMR) project \cite{AMR:13}, which has attracted considerable attention recently, also provides semantic annotation in the form of elaborate graphs, representing distinctions of various types (including word sense disambiguation, argument structure, compositionality, co-reference). AMR is not optimal for our purposes as it conflates a wide variety of linguistic phenomena into a single unlayered representation, and reflects highly fine-grained phenomena, resulting in many cross-linguistic divergences \cite{XUE14not}.
%AMR poses further difficulties in not embedding the text within the semantic representation, often requires that this correspondence will be automatically (and hence imprecisely) derived.
%(e.g., \cite{flanigan2014discriminative}).

%\subsubsection{Universal Cognitive Conceptual Annotation}

%The inadequacy of string-to-string methods in SMT has resulted in structure aware methods that construe translation as the learning of a function between texts augmented with structural annotation, generally relfecting their syntactic or other structural propoerties. In this sense, imposing linguistic structure on the source and target texts is useful if the translation function between these structures is sufficiently constrained to allow its statistical learning. Semantic representation is an appealing source of cross-linguistically applicable structures, since a major goal of translation is preserving the meaning of the source text.

%UCCA shares AMR's [isn't AMR English-centric?] goal of a cross-linguistically applicable scheme that can benefit semantics-based MT, but is better suited for our purposes in two respects. First, UCCA builds on typological theory and provides empirical support for its cross-linguistic applicability through its application to a sizable corpora in English and French, using the same set of categories \cite{sulem2015conceptual}, in addition to further preliminary experiments in German, Hebrew and Czech. Second, it is built in {\it layers}, each addressing a specific module of semantic distinctions, such as argument-structural, information-theoretic or logical distinctions, which allows more controlled experiments, important in these early stages of semantics-based MT.



%Syntactic dependency trees can in general be non-projective. Graph-based parsers are able to produce such trees, but many transition-based parsers assume projectivity to improve the performance and efficiency of the parser: the number of transitions in such parsers is always $2n$ where $n$ is the number of tokens \cite{nivre2004incrementality}. However, non-projective dependency trees can also be parsed by transition-based parsers in empirical linear time \cite{nivre2009non}.

%In a UCCA graph, labels appear on the edges, whereas nodes are unlabeled. If all the nodes are known, parsing is the same as inducing a graph on them with the correct edge labels. In dependency parsing, a graph is induced on the set of nodes consisting of the tokens in the sentence (and the \textsc{ROOT} symbol). Therefore, it seems that techniques from dependency parsing can be used for UCCA parsing as well.

%\paragraph{Other Semantic Schemes.}

%Although the labels in a UCCA graph are on the edges, it is similar to phrase structure grammar: non-terminal units are internal nodes in the graph, rather than all the edges being between words in the original sentence. In a constituency tree, constituents form a hierarchy above the words of the sentence.

%Methods for constituent parsing can also be useful for UCCA parsing, but the common chart-based approach is also $\mathcal{O}(n^3)$ at best. However, there are also transition-based constituent parsers~\cite{zhu2013fast} with linear run-time complexity.

%Like standard dependency parsers, transition-based constituent parsers are generally unable to produce \textit{discontinuous constituents} (the equivalent of non-projective dependency trees). However, Maier~\shortcite{maier2015discontinuous} introduced a transition-based constituent parsing with a \textsc{swap} transition to allow discontinuous parsing.\footnote{https://github.com/wmaier/uparse}


\subsection{Semantic Constituency Parsing}

For a given set $L$ of edge labels, a \textit{semantic constituency graph} $G=(V,E, \ell)$ over a sequence of tokens $w_1, \ldots, w_n$ is a directed acyclic graph (DAG), where $V$ is a set of \textit{nodes}, $E \subseteq V\times V$ is a set of \textit{edges}, and $\ell:E\to L$ is a function of \textit{edge labels}. For each token $w_i$ ($i=1, \ldots, n$) there exists a leaf  $t_i \in V$. {\it Terminal units} are defined as the set of leaf descendants of the nodes in $V$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conversion-Based Parsing}\label{sec:conversion_approach}

Conversion-based parsers are obtained by converting the semantic constituency graphs into the mediating representation,
training a parser for the mediating representation, applying the resulting parser
to the test set (converted to the mediating representation as well), and converting back
to the original semantic constituency graphs.

\paragraph{Conversion to Constituency Trees.}
In order to convert semantic constituency graphs to constituency trees, we remove edges such that
all nodes are left with a single incoming edge, and a single root remains in the graph.
This results in a tree that can be parsed by constituency parsers.
As semantic constituency graphs may have discontiguous units, the resulting tree may have
discontiguous constituents as well.

Specifically, in converting UCCA structures, we note that UCCA marks one of the incoming edges
for each non-root as ``primary'' and the others as ``remote'' and ``linkage'' edges.
We thus remove all remote remote and linkage edges, in addition to nodes that are disconnected
from the graph as a result of this removal, and leaves that do not correspond to a token in the
text\footnote{Such implicit units serve in UCCA to represent implicit relations and participants, such as
the agent in passive sentences (if not explicitly mentioned).}.

The reverse conversion from trees to graphs is simply the identity function.

We use the \textsc{uparse} parser \cite{maier2015discontinuous}, the only transition-based
constituency parser, to our knowledge, able to parse trees with discontiguous constituents.

\paragraph{Conversion to Dependency Trees.}\label{subsec:con2dep}
In the conversion to dependency trees, we first convert semantic constituency graphs to constituency trees using the above procedure. We then convert these constituency trees to dependency trees. The conversion and reverse conversion algorithms are given in Algorithms~\ref{alg:con2dep} and~\ref{alg:dep2con}.

In the conversion from constituency to dependency, we need to remove the non-terminals from the tree and connect the terminals themselves with edges.
We define a priority order over the possible edge labels.
For each node $u \in V$, denote by $h(u)$ its child with the highest-priority edge label.
We then define $h^*(u)$ as the terminal reachable from $u$ by following only highest-priority edges.
For each terminal $t$, we define $n(t)$ as the highest non-terminal $u$ such that $t=h^*(u)$.
The nodes of the converted dependency tree are just the terminal of the constituency tree,
and the edge are then defined in a similar manner to head percolation rules used in the conversion of syntactic treebanks.
However, the rules used here are simpler, as the head child $h(u)$ of a node $u$ depends only on the label of the edge between them, and not on the edges below $u$. This is appropriate, since edges in a semantic constituency graph express the relation between units, whereas in syntactic constituency trees the identity of a node depends on the nodes below it.

\begin{algorithm}[t]
 \KwData{constituency tree ${T_c}=(V_c,E_c,\ell_c)$}
 \KwResult{dependency tree $T_d=(V_d,E_d,\ell_d)$}
 \ForEach{$u \in V_c$} {
  $h(u) \leftarrow \argmin_v \mathrm{Priority}(\ell_c(u,v))$\;
 }
 $V_d \leftarrow \mathrm{Terminals}({T_c})$,
 $E_d \leftarrow \emptyset$\;
 \ForEach{$t \in V_d$} {
  $u \leftarrow t$\;
  \While{$u=h(\mathrm{Parent}_c(u))$} {
  	$h^*(u) \leftarrow t$\;
  	$u \leftarrow \mathrm{Parent}_c(u)$\;
  }
  $n(t) \leftarrow h(u)$\;
 }
 \ForEach{$t \in V_d$} {
  $u \leftarrow \mathrm{Parent}_c(n(t))$\;
  $t^\prime \leftarrow h^*(u)$\;
  $E_d \leftarrow E_d \cup \{(t^\prime, t)\}$\;
  $\ell_d (t^\prime, t) \leftarrow \ell_c(u, n(t))\}$\;
 }
 \caption{Constituency to dependency.}
 \label{alg:con2dep}
\end{algorithm}

In the conversion from dependency to constituency, we need to introduce the non-terminal nodes back.
First, we add an artificial root node $r$.
Iterating through the dependency nodes in topological order, we add them as terminals to the constituency tree and create pre-terminal parents for them, with a \textsc{Terminal}-labeled edge between them. We then attach the pre-terminal to its parent in the constituency tree: if the dependency node is a child of the root, then the constituency node will also be a child of the root node. Otherwise, we attach it to the pre-terminal node associated with the dependency node's head.
We define the labels for newly-introduced edges according to a function defined by heuristic rules, $\mathrm{Label}(u)$. For example, in UCCA, if a node has a sibling that is annotated as a \textsc{ParallelScene}, we annotate it as a \textsc{ParallelScene} too.

\begin{algorithm}
 \KwData{dependency tree $T_d=(V_d,E_d,\ell_d)$}
 \KwResult{constituency tree $T_c=(V_c,E_c,\ell_c)$}
 $r \leftarrow \mathrm{Node()}$\;
 $V_c \leftarrow \{r\}$,
 $E_c \leftarrow \emptyset$\;
 \ForEach{$t \in \mathrm{TopologicalSort}(V_d)$} {
  $u \leftarrow \mathrm{Node()}$\;
  $V_c \leftarrow V_c \cup \{u, t\}$\;
  $E_c \leftarrow E_c \cup \{(u, t)\}$\;
  $\ell_c(u,t)\leftarrow\textsc{Terminal}$\;
  \eIf{$\ell_d(t) = \textsc{Root}$} {
   $E_c \leftarrow E_c \cup \{(r, u)\}$\;
   $\ell_c(r, u) \leftarrow \mathrm{Label}(r,u)$\;
  } {
   $p \leftarrow \mathrm{Parent}_c(\mathrm{Parent}_d(t))$\;
   $E_c \leftarrow E_c \cup \{(p, u)\}$\;
   $\ell_c(p, u) \leftarrow \mathrm{Label}(p,u)$\;
  }
 }
 \caption{Dependency to constituency.}
 \label{alg:dep2con}
\end{algorithm}

We use MaltParser \cite{nivre2007maltparser} and the stack LSTM-based parser by Dyer et al.~\shortcite{dyer2015transition} as the underlying parsers in this method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Constituency DAG Parsing}\label{sec:direct_approach}

We now turn to the direct approach for parsing semantic constituency graphs. Given a sequence
of tokens $w_1, \ldots, w_n$, we directly predict a DAG $G$ whose leaves
correspond to  $w_1, \ldots, w_n$.

%We define a transition system for constituency DAG parsing as a quadruple $S=(C,T,c_s,C_t)$, where
%\begin{enumerate}
% \item $C$ is a set of \textit{configurations}.
% \item $T$ is a set of \textit{transitions}.
% \item $c_s$ is a function mapping a sequence of tokens to an \textit{initial state} in $C$.
% \item $C_t$ is a set of \textit{terminal configurations}.
%\end{enumerate}

In transition-based (also called \textit{shift-reduce}) parsing, the input is scanned from left to right.
The output is created incrementally by applying a \textit{transition} to the parser state,
defined using three data structures: a buffer $B$ of tokens and nodes to be processed,
a stack $S$ of nodes currently being processed,
and a graph $G=(V,E)$ of constructed nodes and edges.
In addition, some states are marked as \textit{terminal}, meaning that $G$ is the final output.
A classifier is used at each step to select the next transition based on features
which encode the parser's current state.
Following \newcite{maier2015discontinuous}, we use a linear classifier, using
the averaged structured perceptron algorithm for training it
\cite{Coll:04} with the \textsc{MinUpdate} \cite{cai2011language} procedure
(i.e., a minimum number of updates to a feature has to oocur in training for it
to be included in the model).

\paragraph{Transition Set.}
Parsing starts with a single node in the stack (the root node), and the input tokens
$w_1, \ldots, w_n$ in the buufer $B$. The transition set is given in \figref{fig:transitions}.

\begin{figure*}
\captionsetup{justification=centering}
\begin{adjustbox}{width=\textwidth,margin=3pt,frame}
\begin{tabular}{llll|l|llllc|c}
\multicolumn{5}{c|}{\textbf{\small Initial State}} & \multicolumn{6}{c}{\textbf{\small Final State}} \\
\textbf{\footnotesize Stack} & \textbf{\footnotesize Buffer} & \textbf{\footnotesize Nodes} & \multicolumn{1}{l}{\textbf{\footnotesize Edges}} & \multicolumn{1}{c|}{\textbf{\footnotesize Terminal?}} & \textbf{\footnotesize Stack} & \textbf{\footnotesize Buffer} & \textbf{\footnotesize Nodes} & \textbf{\footnotesize Edges} & \multicolumn{1}{c}{\textbf{\footnotesize Terminal?}} \\
$[\mathrm{root}]$ & $w_{1:n}$ & \multirow{2}{40pt}{$\{\mathrm{root}\} \cup w_{1:n}$} & \multicolumn{1}{l}{$\emptyset$} & \multicolumn{1}{c|}{$-$} & $\emptyset$ & $\emptyset$ & $V$ & $E$ & \multicolumn{1}{c}{$+$} \\ 
\multicolumn{5}{c|}{} \\
\multicolumn{5}{c|}{} \\
\hline
%\end{tabular}
%\end{adjustbox}
%\begin{adjustbox}{width=\textwidth,frame}
%\begin{tabular}{llll|l|llllc|l}
\multicolumn{4}{c|}{\textbf{\small Before Transition}} & \textbf{\small Transition} & \multicolumn{5}{c|}{\textbf{\small After Transition}} & \textbf{\small Condition} \\
\textbf{\footnotesize Stack} & \textbf{\footnotesize Buffer} & \textbf{\footnotesize Nodes} & \textbf{\footnotesize Edges} & & \textbf{\footnotesize Stack} & \textbf{\footnotesize Buffer} & \textbf{\footnotesize Nodes} & \textbf{\footnotesize Edges} & \textbf{\footnotesize Terminal?} & \\
$S$ & $x \;|\; B$ & $V$ & $E$ & \textsc{Shift} & $S \;|\; x$ & $B$ & $V$ & $E$ & $-$ & \\
$S \;|\; x$ & $B$ & $V$ & $E$ & \textsc{Reduce} & $S$ & $B$ & $V$ & $E$ & $-$ & \\
\cline{11-11}
$S \;|\; x$ & $B$ & $V$ & $E$ & \textsc{Node$_X$} & $S \;|\; x$ & $y \;|\; B$ & $V \cup \{ y \}$ & $E \cup \{ (y,x)_X \}$ & $-$ &
$x \neq \mathrm{root}$ \\
\cline{11-11}
$S \;|\; x$ & $B$ & $V$ & $E$ & \textsc{Implicit$_X$} & $S \;|\; x$ & $y^* \;|\; B$ & $V \cup \{ y^* \}$ & $E \cup \{ (x,y^*)_X \}$ & $-$ &
$x \not\in w_{1:n}$ \\
\cline{11-11}
$S \;|\; y,x$ & $B$ & $V$ & $E$ & \textsc{Left-Edge$_X$} & $S \;|\; y,x$ & $B$ & $V$ & $E \cup \{ (x,y)_X \}$ & $-$ &
\multirow{4}{50pt}{$\begin{aligned}
x \not\in w_{1:n},\\
y \neq \mathrm{root},\\
x \not\leadsto_G y
\end{aligned}$} \\
$S \;|\; x,y$ & $B$ & $V$ & $E$ & \textsc{Right-Edge$_X$} & $S \;|\; x,y$ & $B$ & $V$ & $E \cup \{ (x,y)_X \}$ & $-$ & \\
$S \;|\; y,x$ & $B$ & $V$ & $E$ & \textsc{Left-Remote$_X$} & $S \;|\; y,x$ & $B$ & $V$ & $E \cup \{ (x,y)_X^* \}$ & $-$ & \\
$S \;|\; x,y$ & $B$ & $V$ & $E$ & \textsc{Right-Remote$_X$} & $S \;|\; x,y$ & $B$ & $V$ & $E \cup \{ (x,y)_X^* \}$ & $-$ & \\
\cline{11-11}
$S \;|\; x,y$ & $B$ & $V$ & $E$ & \textsc{Swap} & $S \;|\; y$ & $x \;|\; B$ & $V$ & $E$ & $-$ &
$\mathrm{i}(x) < \mathrm{i}(y)$ \\
\cline{11-11}
$[r]$ & $\emptyset$ & $V$ & $E$ & \textsc{Finish} & $\emptyset$ & $\emptyset$ & $V$ & $E$ & $+$ & \\
\end{tabular}
\end{adjustbox}
\caption{\label{fig:transitions}
Transitions for UCCA parsing.
As usual, we write the stack with its head (top) to the right and the buffer with its head to the left.
$(\cdot,\cdot)_X$ denotes an $X$-labeled edge, $(\cdot,\cdot)_X^*$ a remote $X$-labeled edge, and $x^*$ an implicit node.
$\mathrm{i}(x)$ is a running index for created nodes.
}
\end{figure*}

In addition to the standard \textsc{Shift} and \textsc{Reduce} operations, 
we follow previous work in transition-based constituency parsing \cite{zhu2013fast}, and include the \textsc{Node} transition for creating new non-terminal nodes.
Concretely, \textsc{Node$_X$} creates a new node on the buffer as a parent of the first element on the stack, with an $X$-labeled edge. \textsc{Implicit$_X$} acts similarly, but the created node is a child instead,
and is marked as \textit{implicit}, representing a unit that does not appear explicitly in the text.

\textsc{Left-Edge$_X$} and \textsc{Right-Edge$_X$} create a new $X$-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. \textsc{Left-Remote$_X$} and \textsc{Right-Remote$_X$} do the same, but the created edge is marked as \textit{remote}, meaning the relation it represents is not explicit.
In order to allow nodes to have multiple parents, all node and edge transitions leave the
stack and buffer as they are and do not reduce them,
as in work on transition-based DAG dependency parsing \cite{sagae2008shift,tokgoz2015transition}.
Once all edges for a particular node have been created, it is removed from the stack
by the transition \textsc{Reduce}.

\textsc{Swap} allows to handle discontinuous nodes, by popping the second
node on the stack and addding it to the top of the buffer, similar to a
similarly named transition in \cite{maier2015discontinuous}.
Finally, \textsc{Finish} pops the root node and marks the state as terminal.

\subsection{Features}
\label{subsec:features}

\figref{fig:features} presents the feature templates used by the parser.
For some of the features, we used the notion of \textit{head word}, defined 
as the first terminal reachable by following outgoing edges following a priority order
over the edge labels (similar to the one used by the conversion in \secref{subsec:con2dep}\oa{the same one?}).
While head words is not explicitly represented in the UCCA scheme, these
features proved beneficial as means of encoding bi-lexical features.

In addition to the binary features defined by those feature templates,
we employ a real-valued feature corresponding to the number of terminals to number of nodes
ratio in the graph $G$. The feature serves as a regularizer for the creation of new nodes.\oa{did you take it
  from somewhere?}
Features are generally adapted from the related parsers of \newcite{zhu2013fast,tokgoz2015transition} and
\newcite{maier2015discontinuous}, with a small additional set of features encoding relevant information
for the novel \textsc{Left-Remote$_X$}, \textsc{Right-Remote$_X$} and \textsc{IMPLICIT$_X$} operations.
\oa{what information exactly? Figure 3 is a bit disorganized. Better to put all the notation in the caption.}

%$s_i$ and $b_i$ stand for the $i$th stack and buffer items respectively, $w$ stands for the head word, $t$ for its POS tag, $e$ for the head incoming edge label. Note that $e$ replaces $c$ (consituent label) from Maier~\shortcite{maier2015discontinuous}, since in UCCA the labels are on the edges rather than the units; and a unit may have more than one incoming edge. $l$ and $r$ ($ll$ and $rr$) represent the leftmost and rightmost children (grand-children) of the element; $u$ handles the unary case.
%Concerning the separator features, $p$ is a unique separator feature between the head words of $s_0$ and $s_1$; $q$ is the count of any separator features between them.
%$x$ denotes the gap type of a subgraph. There are three possible values, either "none" (fully continuous), "pass" (there is a gap at the root, i.e., this gap must be filled later further up in the graph), or "gap" (the root of this graph fills a gap, i.e., its children have gaps, but the root does not). Finally, $y$ is the sum of all gap lengths.

% FEATURES
\begin{figure*}
\captionsetup[subfigure]{justification=centering}
\begin{adjustbox}{width=\textwidth}
\small
\begin{minipage}[c][11cm][t]{.5\textwidth}
\vspace*{\fill}
\begin{tabular}{l}
\textbf{unigrams} \\
$s_0te, s_0we, s_1te, s_1we, s_2te, s_2we, s_3te, s_3we,$ \\
$b_0wt, b_1wt, b_2wt, b_3wt,$ \\
$s_0lwe, s_0rwe, s_0uwe, s_1lwe, s_1rwe, s_1uwe$ \\
\textbf{bigrams} \\
$s_0ws_1w, s_0ws_1e, s_0es_1w, s_0es_1e, s_0wb_0w, s_0wb_0t,$ \\
$s_0eb_0w, s_0eb_0t, s_1wb_0w, s_1wb_0t, s_1eb_0w, s_1eb_0t,$ \\
$b_0wb_1w, b_0wb_1t, b_0tb_1w, b_0tb_1t$ \\
\textbf{trigrams} \\
$s_0es_1es_2w, s_0es_1es_2e, s_0es_1eb_0w, s_0es_1eb_0t,$ \\
$s_0es_1wb_0w, s_0es_1wb_0t, s_0ws_1es_2e, s_0ws_1eb_0t$ \\
\textbf{extended} \\
$s_0llwe, s_0lrwe, s_0luwe, s_0rlwe, s_0rrwe,$ \\
$s_0ruwe, s_0ulwe, s_0urwe, s_0uuwe, s_1llwe,$ \\
$s_1lrwe, s_1luwe, s_1rlwe, s_1rrwe, s_1ruwe$ \\
\textbf{separator} \\
$s_0wp, s_0wep, s_0wq, s_0wcq, s_0es_1ep, s_0es_1eq,$ \\
$s_1wp, s_1wep, s_1wq, s_1weq$
\end{tabular}
\subcaption{Features from Zhu et al.~\shortcite{zhu2013fast}. \\
\footnotesize
$s_i$, $b_i$: stack and buffer items. \\
$w$: head word form. \\
$t$: head word part-of-speech tag. \\
$e$: head incoming edge label. \\
$l$, $r$ ($ll$, $rr$): leftmost, rightmost (grand)children. \\
$u$ ($uu$): unary (grand)child. \\
$p$: unique separator between $s_0$ and $s_1$. \\
$q$: separator count.}
\end{minipage}

\begin{minipage}[c][11cm][t]{.5\textwidth}
\vspace*{\fill}
\begin{tabular}{l}
\textbf{unigrams} (discontinuous) \\
$s_0xwe, s_1xwe, s_2xwe, s_3xwe,$ \\
$s_0xte, s_1xte, s_2xte, s_3xte,$ \\
$s_0xy, s_1xy, s_2xy, s_3xy$ \\
\textbf{bigrams} (discontinuous) \\
$s_0xs_1e, s_0xs_1w, s_0xs_1x, s_0ws_1x, s_0es_1x,$ \\
$s_0xs_2e, s_0xs_2w, s_0xs_2x, s_0ws_2x, s_0es_2x,$ \\
$s_0ys_1y, s_0ys_2y, s_0xb_0t, s_0xb_0w$
\end{tabular}
\subcaption{Features from Maier~\shortcite{maier2015discontinuous}. \\
\footnotesize
$x$: gap type. \\
$y$: sum of gap lengths.}

\vspace*{\fill}
\begin{tabular}{l}
\textbf{counts} \\
$s_0P, s_0C, s_0wP, s_0wC,$ \\
$b_0P, b_0C, b_0wP, b_0wC$ \\
\textbf{existing edges} \\
$s_0s_1, s_1s_0, s_0b_0, b_0s_0,$ \\
$s_0b_0e, b_0s_0e$ \\
\textbf{past actions} \\
$a_0we, a_1we$
\end{tabular}
\subcaption{Features from Tokg{\"o}z and Eryi\u{g}it~\shortcite{tokgoz2015transition}. \\
\footnotesize
$P,C$: number of parents, children. \\
$a_i$: past actions (in reverse order).}

\vspace*{\fill}
\begin{tabular}{l}
\textbf{implicit and remote} \\
$s_0I, s_0R, s_0wI, s_0wR,$ \\
$b_0I, b_0R, b_0wI, b_0wR$
\end{tabular}
\subcaption{New, UCCA-specific features. \\
\footnotesize
$I,R$: number of implicit, remote children.}
\end{minipage}
\end{adjustbox}
\caption{Feature Templates.\label{fig:features}}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\subsection{Data}\label{sec:data}

We base our experiments on the UCCA Wiki corpus as the main corpus, the UCCA \textit{Twenty Thousand Leagues Under the Sea} corpus as out-of-domain data. As evident in \tabref{table:data}, the Wiki corpus is over ten times larger, but the overall statistics are similar.
\oa{Give examples of cases where multiple parents and/or discontiguous units show up.}
\oa{put statistics here similar to table 1 in the semeval 2015 paper; specifically, treewidth may be interesting}

\begin{table}
\begin{tabular}{l|ccc}
& Wiki & o-o-d \\
\hline
\multicolumn{3}{l}{\footnotesize Over whole corpus} \\
\# terminal nodes & 158,771 & 12,355 \\
\# nodes & 379,051 & 30,487 \\
\# edges & 388,589 & 31,215 \\
\# remote edges & 5,745 & 629 \\
\# non-terminal nodes & 220,280 & 18,132 \\
\# implicit nodes & 1,099 & 241 \\
%\% non-terminals & 58.11 & 59.47 \\
\multicolumn{3}{l}{\footnotesize Out of edges} \\
\% remote & 1.48 & 2.02 \\
\multicolumn{3}{l}{\footnotesize Out of non-terminals} \\
\% multiple parents & 7.66 & 7.91 \\
\% discontinuous & 0.88 & 1.31 \\
\% implicit & 0.5 & 1.33 \\
\multicolumn{3}{l}{\footnotesize Average over non-terminals} \\
\# parents & 1.07 & 1.07 \\
\# children & 1.7 & 1.63 \\
\end{tabular}
\caption{High-level graph statistics in the UCCA corpora.}
\label{table:data}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Setup}

Our conversion tools and direct parser are implemented in Python. In all experiments, we use passages up to 655 of the UCCA Wiki corpus as our training set, passages 656-700 as development set, and passages 701-695 as test set. We split the passages to sentences, such that each UCCA graph is annotated on one sentence. In the out-of-domain experiments, we use the same training and development sets, but use the whole UCCA \textit{Twenty Thousand Leagues Under the Sea} corpus as test set.z`
For evaluation, we use the \texttt{evaluate\_standard.py} script, and report the labeled precision, recall and F1 when comparing the edges present in the predicted graph against the gold-standard graph. We report the scores on regular (non-remote) and remote edges separately.

For the conversions, we use \textsc{NeGra export} as the format for constituency representation and \textsc{CoNLL-X} as the format for dependency representation.

For MaltParser, we use \texttt{nivreeager} as the transition system and \texttt{libsvm} as the classifier, as it reaches the highest scores (although with a close margin) among configurations.

\oa{this belongs in the exp setup}
Graphs produced by this method for test data are then compared to the gold annotation.
We also perform oracle experiments, where instead of a trained parser's output we use the
gold annotation itself, converted to the target representation
(and then converted back to semantic constituencies).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}

\tabref{table:convert} shows the evaluation metrics on the different methods.
The conversion to constituency format gets almost a perfect score on the regular edges, as it only omits extra linkage parents and implicit nodes. The conversion to dependency format is more lossy, since all of the non-terminal nodes are lost and have to be reconstructed on the conversion back. Of course, both conversions have a zero score on remote edges, since they are always removed when converting to trees.

Our parser reaches scores that are nearly identical to Maier~\shortcite{maier2015discontinuous} and Nivre et al.~\shortcite{nivre2007maltparser} on regular edges, but also correctly predicts remote edges when trained on them.
Training without linkage edges and implicit nodes improve the scores on regular edges, as a more simple model is easier to learn. Furthermore, removing remote edges too practically turns the structure into a constituency tree, and raises the scores above two of the conversion-based parsers. However, the stack LSTM-based parser of Dyer et al.~\shortcite{dyer2015transition} still reaches the highest score.

\begin{table*}[ht]
\begin{tabular}{l|ccc|ccc}
\daniel{These scores need to be updated to use the modified evaluation script}
& \multicolumn{3}{c|}{Regular} & \multicolumn{3}{c}{Remote} \\
& \textbf{LP} & \textbf{LR} & \textbf{LF} & \textbf{LP} & \textbf{LR} & \textbf{LF} \\
\hline
\multicolumn{4}{l}{\footnotesize Constituency format} \\
\textit{Oracle} & 100 & 99.5 & 99.7 & $-$ & 0 & 0 \\
Maier~\shortcite{maier2015discontinuous} & 58 & 59.8 & 58.7 & $-$ & 0 & 0 \\
\hline
\multicolumn{4}{l}{\footnotesize Dependency format} \\
\textit{Oracle} & 93.7 & 86 & 89.6 & $-$ & 0 & 0 \\
Nivre et al.~\shortcite{nivre2007maltparser} & 61.1 & 56.5 & 58.7 & $-$ & 0 & 0 \\
Dyer et al.~\shortcite{dyer2015transition} & 71.8 & 67 & 69.3 & $-$ & 0 & 0 \\
\hline
\multicolumn{4}{l}{\footnotesize Direct approach} \\
$S$ &  &  & 56.5 &  &  & 18.5 \\
$S_{LI}$ &  &  & 58 &  &  & 16.6 \\
$S_{LIR}$ &  &  & 60.2 &  &  & 0 \\
$S$ (o-o-d) &  &  & 53.6 &  &  & 8.5 \\
$S_{LI}$ (o-o-d) &  &  & 55.8 &  &  & 13.1 \\
\end{tabular}
%\begin{adjustbox}{width=\textwidth}
%\pgfplotstabletypeset[col sep=comma,
%     columns={Method,LAS,LAS (sentences),UAS,UAS (sentences),labeled F1,labeled F1 (sentences),unlabeled F1,unlabeled F1 (sentences),weak labeled F1,weak labeled F1 (sentences)},
%     columns/Method/.style={string type, column type=l,
%         postproc cell content/.code={%
%             \pgfplotsutilstrreplace{_}{\_}{##1}%
%             \pgfkeyslet{/pgfplots/table/@cell content}\pgfplotsretval
%         },}
%    ]{results.csv}
%\end{adjustbox}
\caption{Parsing scores.
LP, LR and LF are labeled precision, recall and F1, respectively.
$S$ is our parser when trained on the whole data set; $S_{LI}$ is our parser when trained without linkage edges and implicit nodes, and $S_{LIR}$ is without linkage, implicit and remote.
o-o-d is out-of-domain.}
\label{table:convert}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

We have presented the task Broad-Coverage Semantic Constituency parsing (BSCP), and shown that the Universal Conceptual Cognitive Annotation (UCCA) scheme is a target representation for this task. Based on high-level statistics over two UCCA corpora, we demonstrated that they are indeed structurally unrestricted.
We claim UCCA is more appropriate than other semantic parsing tasks, since it is not bounded by arbitrary head selection rules, and is grounded on the tokens of the text.

We present two approaches for UCCA parsing and BSCP in general: conversion to constituency and dependency formats, and a direct transition-based parsing approach with a novel transition system.
The parsers we evaluated on this tasks vary on several parameters: transition system and classifier. \tabref{table:parser_variation} shows the different parameters of each of these parsers.

\begin{table*}[ht]
\begin{tabular}{l|ll}
Parser & Transition System & Classifier \\
\hline
Maier~\shortcite{maier2015discontinuous} & constituency + \textsc{Swap} & linear \\
Nivre et al.~\shortcite{nivre2007maltparser} & arc-eager & SVM \\
Dyer et al.~\shortcite{dyer2015transition} & arc-standard & LSTM \\
Our work & constituency DAG + \textsc{Swap} & linear
\end{tabular}
\caption{Parameters of variation across parsers evaluated in this work.}
\label{table:parser_variation}
\end{table*}

\subsection{Future Work}
\paragraph{Improving the conversion procedure.}
Several decision have to be made in the conversion between dependency and constituency annotation.
In the conversion from constituency to dependency, a head terminal has to be selected for each node. When converting in the other direction, labels for the added edges have to be determined.
Currently, we handle these decisions by crude heuristics, but a more sophisticated way can be devised. These will be reflected by higher scores for the oracle experiments.

\paragraph{Improving the direct parser.}
Our parser reaches promising results, but it is very basic in terms of the techniques used in it. Using beam search rather than greedy inference tends to improve the parsing scores significantly.
In terms of machine learning, we use a linear classifier with hand-crafted features. As the experiment with the LSTM parser shows, implementing a parser based on neural networks should also give a significant improvement to our parsing scores.

\bibliography{references}
\bibliographystyle{acl2016}

\end{document}


\com{
  (8) we expriment with the UCCA corpus...
  (7) since this is the first parser for the task, we compare the results to partial
  implementations of the algorithm. we also conduct a comparison with existing parsers for similar
  parsing tasks.
  (9) our contribution: \#1 defining the 
  \#2 first parser for broad-coverage constituency-based parsing; \#3 first parser for UCCA;
  Previous work: the work has interesting connections to syntactic parsing, to SRL (which also has both dependency
  and constituency versions), and to semantic parsing into logical forms, but is different from the,
  see discussion in (Oepen et al., 2015). Also AMR parsing.

Universal Conceptual Cognitive Annotation (UCCA) is a recently introduced semantic annotation scheme. It takes a semantic approach to grammatical representation, describing relations between words and phrases in natural language text. The foundational layer can potentially be extended by any number of additional layers to provide refinements to the existing scheme, but it already covers many of the basic semantic relations in language. The annotation is represented as a directed acyclic graph (DAG), where edges denote semantic dependencies between abstract units. UCCA can be easily annotated manually without expert knowledge in linguistics, making it attractive as a resource for semantic tasks---an alternative to syntactic annotation, which is costly. A corpus containing 160K words from the English Wikipedia has been manually annotated~\cite{abend2013universal}.

UCCA exhibits three properties that are generally absent in syntactic schemes:
\begin{description}
\item[Multiple parents.] In general, the UCCA graph is a DAG and not a tree.
\item[Discontinuity.] The yield of a node may have gaps.
\item[Non-terminal units.] There are units representing abstract entities or events, and not all nodes correspond to tokens.
\end{description}

This work is the first attempt at automatic UCCA parsing.
}
